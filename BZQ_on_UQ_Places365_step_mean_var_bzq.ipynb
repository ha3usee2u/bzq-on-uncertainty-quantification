{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.interpolate import interp1d\n",
    "import tensorflow_datasets as tfds\n",
    "from collections import Counter\n",
    "import scipy.ndimage\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "import gc\n",
    "import random\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import gaussian_kde\n",
    "from numba import jit\n",
    "from torchvision.datasets import Places365\n",
    "from tqdm import tqdm  # 導入進度條庫\n",
    "import json\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "limit = 1000\n",
    "\n",
    "# bzq modifying\n",
    "len_x_target = 20\n",
    "len_y_target = 20\n",
    "stride_x_target = 10\n",
    "stride_y_target = 10\n",
    "\n",
    "# mean, std proportion\n",
    "alpha = 0.5\n",
    "\n",
    "bins_size = 30  # 統計採樣數\n",
    "poly_degree = bins_size - 1\n",
    "window_size = 1\n",
    "\n",
    "#target image preprocessing\n",
    "angle = 0\n",
    "pixels = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設置設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),  # Convert to tensor (values in range 0–1)\n",
    "    transforms.Lambda(lambda x: x * 255.0),  # Scale to range 0–255\n",
    "    transforms.Lambda(lambda x: x[[2, 1, 0], ...]),  # Convert RGB to BGR\n",
    "    transforms.Normalize(mean=[103.939, 116.779, 123.68], std=[1.0, 1.0, 1.0])  # Subtract mean\n",
    "])\n",
    "# 載入 Places365 數據集\n",
    "dataset = Places365(\n",
    "    root=\"/home/a/bzq_on_confidence/Confidence2022/notebook/confident/grocery/Places365\",\n",
    "    split=\"train-standard\",  # 指定數據集分割，例如 \"train-standard\", \"val\"\n",
    "    small=True,             # 是否使用小尺寸圖像（256x256）\n",
    "    transform=transform,     # 圖像增強\n",
    "    target_transform=None,   # 標籤增強\n",
    "    download=True            # 自動下載數據集\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adjust_learning_rate(optimizer, epoch, initial_lr):\n",
    "    if epoch <= 5:\n",
    "        lr = initial_lr * epoch / 5\n",
    "    elif epoch > 160:\n",
    "        lr = initial_lr * 0.01\n",
    "    elif epoch > 75:\n",
    "        lr = initial_lr * 0.1\n",
    "    else:\n",
    "        lr = initial_lr\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# Define the VGG16 model\n",
    "model = models.vgg16(weights=False)  # Initialize without pre-trained weights\n",
    "model = model.to(device)\n",
    "num_features = model.classifier[6].in_features  # Get the input features of the last FC layer\n",
    "model.classifier[6] = torch.nn.Linear(num_features, 365)  # Modify the last layer for 365 classes\n",
    "\n",
    "model.features = model.features.to(device)\n",
    "model.classifier = model.classifier.to(device)\n",
    "\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "num_epochs = 200\n",
    "\n",
    "if os.path.exists(f\"vgg16_places365.pt\"):\n",
    "    # 加載保存的權重\n",
    "    checkpoint = torch.load(\"vgg16_places365.pt\")\n",
    "\n",
    "    # Mapping Caffe keys to PyTorch keys\n",
    "    mapped_state_dict = {\n",
    "        # Conv1 Layers\n",
    "        \"features.0.weight\": checkpoint[\"conv1_1.weight\"],\n",
    "        \"features.0.bias\": checkpoint[\"conv1_1.bias\"],\n",
    "        \"features.2.weight\": checkpoint[\"conv1_2.weight\"],\n",
    "        \"features.2.bias\": checkpoint[\"conv1_2.bias\"],\n",
    "        \n",
    "        # Conv2 Layers\n",
    "        \"features.5.weight\": checkpoint[\"conv2_1.weight\"],\n",
    "        \"features.5.bias\": checkpoint[\"conv2_1.bias\"],\n",
    "        \"features.7.weight\": checkpoint[\"conv2_2.weight\"],\n",
    "        \"features.7.bias\": checkpoint[\"conv2_2.bias\"],\n",
    "        \n",
    "        # Conv3 Layers\n",
    "        \"features.10.weight\": checkpoint[\"conv3_1.weight\"],\n",
    "        \"features.10.bias\": checkpoint[\"conv3_1.bias\"],\n",
    "        \"features.12.weight\": checkpoint[\"conv3_2.weight\"],\n",
    "        \"features.12.bias\": checkpoint[\"conv3_2.bias\"],\n",
    "        \"features.14.weight\": checkpoint[\"conv3_3.weight\"],\n",
    "        \"features.14.bias\": checkpoint[\"conv3_3.bias\"],\n",
    "        \n",
    "        # Conv4 Layers\n",
    "        \"features.17.weight\": checkpoint[\"conv4_1.weight\"],\n",
    "        \"features.17.bias\": checkpoint[\"conv4_1.bias\"],\n",
    "        \"features.19.weight\": checkpoint[\"conv4_2.weight\"],\n",
    "        \"features.19.bias\": checkpoint[\"conv4_2.bias\"],\n",
    "        \"features.21.weight\": checkpoint[\"conv4_3.weight\"],\n",
    "        \"features.21.bias\": checkpoint[\"conv4_3.bias\"],\n",
    "        \n",
    "        # Conv5 Layers\n",
    "        \"features.24.weight\": checkpoint[\"conv5_1.weight\"],\n",
    "        \"features.24.bias\": checkpoint[\"conv5_1.bias\"],\n",
    "        \"features.26.weight\": checkpoint[\"conv5_2.weight\"],\n",
    "        \"features.26.bias\": checkpoint[\"conv5_2.bias\"],\n",
    "        \"features.28.weight\": checkpoint[\"conv5_3.weight\"],\n",
    "        \"features.28.bias\": checkpoint[\"conv5_3.bias\"],\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        \"classifier.0.weight\": checkpoint[\"fc6.weight\"],\n",
    "        \"classifier.0.bias\": checkpoint[\"fc6.bias\"],\n",
    "        \"classifier.3.weight\": checkpoint[\"fc7.weight\"],\n",
    "        \"classifier.3.bias\": checkpoint[\"fc7.bias\"],\n",
    "        \"classifier.6.weight\": checkpoint[\"fc8a.weight\"],\n",
    "        \"classifier.6.bias\": checkpoint[\"fc8a.bias\"],\n",
    "    }\n",
    "\n",
    "    # Load the updated state dict into the model\n",
    "    model.load_state_dict(mapped_state_dict)\n",
    "    print(model.features)\n",
    "    # 切換模型到評估模式\n",
    "    model.eval()\n",
    "else:\n",
    "    # 獲取數據\n",
    "    image, target = dataset[0]\n",
    "    print(f\"Image size: {image.size()}, Target: {target}\")\n",
    "    \n",
    "    # 計算每個類別的樣本數\n",
    "    targets = [sample[1] for sample in dataset]  # 假設 dataset 返回 (data, label)\n",
    "    class_counts = torch.bincount(torch.tensor(targets))\n",
    "    class_weights = 1.0 / class_counts.float()\n",
    "    weights = torch.tensor([class_weights[label] for label in targets])\n",
    "\n",
    "    # 創建 WeightedRandomSampler\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=weights, \n",
    "        num_samples=len(weights), \n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=80, sampler=sampler, shuffle=False, num_workers=6, pin_memory=True)\n",
    "    val_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=6, pin_memory=True)\n",
    "    print(\"ready\")\n",
    "    \n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        adjust_learning_rate(optimizer, epoch, 0.1)\n",
    "\n",
    "        model.train()  # 設定模型為訓練模式\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "\n",
    "        # 使用 tqdm 包裹 DataLoader，顯示進度條\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as progress_bar:\n",
    "            for images, labels in progress_bar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # 混合精度前向傳播\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                # 縮放損失並進行反向傳播\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # 更新參數\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # 清零梯度\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                # 在進度條中顯示當前損失\n",
    "                progress_bar.set_postfix(loss=f\"{running_loss / batch_count:.4f}\")\n",
    "\n",
    "        # 評估準確度\n",
    "        model.eval()  # 設定模型為評估模式\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():  # 停用梯度計算\n",
    "            for images, labels in val_loader:  \n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)  # 獲取預測標籤\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch + 1}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # 保存模型權重\n",
    "        torch.save(model.state_dict(), f\"Places365_vgg16_epoch_{epoch + 1}.pth\")\n",
    "        print(f\"Model saved for epoch {epoch + 1}\")\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到了bzq 正確的函數，拿來做imagenet 正確的預測\n",
    "\n",
    "def single_data_bzq_mask_preprocessing_imagenet(original_data, start_x, start_y, len_x, len_y, magnification):\n",
    "    if len_x <= 0 or len_y <= 0:\n",
    "        return original_data\n",
    "    new_data = np.copy(original_data)\n",
    "    new_data[:, start_y:start_y + len_y, start_x:start_x + len_x] *= magnification\n",
    "    #new_data[start_y:start_y + len_y, start_x:start_x + len_x, :] *= magnification\n",
    "    return new_data\n",
    "\n",
    "\n",
    "#print(random_num_for_bzq_mask_imagenet)\n",
    "\n",
    "def single_data_bzq_mask_preprocessing_imagenet_random_global(original_data, start_x, start_y, len_x, len_y, random_num_for_bzq_mask_imagenet):\n",
    "    if len_x <= 0 or len_y <= 0:\n",
    "        return original_data\n",
    "    new_data = np.copy(original_data.cpu().numpy())\n",
    "    new_data = new_data.squeeze(0)\n",
    "    random_num_for_bzq_mask_imagenet = random_num_for_bzq_mask_imagenet[:, :len_y, :len_x] \n",
    "    new_data[:, start_y:start_y + len_y, start_x:start_x + len_x] = random_num_for_bzq_mask_imagenet\n",
    "    return new_data\n",
    "\n",
    "\n",
    "bzq = []\n",
    "correct_predictions_imagenet = []\n",
    "incorrect_predictions_imagenet = []\n",
    "bzq_imagenet = []\n",
    "\n",
    "nll_bzq = []\n",
    "\n",
    "#bzq = 0的時候，提取mask之下的softmax\n",
    "correct_predictions_bzq_zero_softmax_mean = []\n",
    "correct_predictions_bzq_zero_softmax_std = []\n",
    "incorrect_predictions_bzq_zero_softmax_mean = []\n",
    "incorrect_predictions_bzq_zero_softmax_std = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Corruptions ---\n",
    "import skimage as sk\n",
    "from skimage.filters import gaussian\n",
    "from io import BytesIO\n",
    "from wand.image import Image as WandImage\n",
    "from wand.api import library as wandlibrary\n",
    "import wand.color as WandColor\n",
    "import ctypes\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "from scipy.ndimage import zoom as scizoom\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "\n",
    "def auc(errs):  # area under the alteration error curve\n",
    "    area = 0\n",
    "    for i in range(1, len(errs)):\n",
    "        area += (errs[i] + errs[i - 1]) / 2\n",
    "    area /= len(errs) - 1\n",
    "    return area\n",
    "\n",
    "\n",
    "def disk(radius, alias_blur=0.1, dtype=np.float32):\n",
    "    if radius <= 8:\n",
    "        L = np.arange(-8, 8 + 1)\n",
    "        ksize = (3, 3)\n",
    "    else:\n",
    "        L = np.arange(-radius, radius + 1)\n",
    "        ksize = (5, 5)\n",
    "    X, Y = np.meshgrid(L, L)\n",
    "    aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n",
    "    aliased_disk /= np.sum(aliased_disk)\n",
    "\n",
    "    # supersample disk to antialias\n",
    "    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n",
    "\n",
    "\n",
    "# Tell Python about the C method\n",
    "wandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n",
    "                                              ctypes.c_double,  # radius\n",
    "                                              ctypes.c_double,  # sigma\n",
    "                                              ctypes.c_double)  # angle\n",
    "\n",
    "\n",
    "# Extend wand.image.Image class to include method signature\n",
    "class MotionImage(WandImage):\n",
    "    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n",
    "        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n",
    "\n",
    "\n",
    "# modification of https://github.com/FLHerne/mapgen/blob/master/diamondsquare.py\n",
    "def plasma_fractal(mapsize=256, wibbledecay=3):\n",
    "    \"\"\"\n",
    "    Generate a heightmap using diamond-square algorithm.\n",
    "    Return square 2d array, side length 'mapsize', of floats in range 0-255.\n",
    "    'mapsize' must be a power of two.\n",
    "    \"\"\"\n",
    "    assert (mapsize & (mapsize - 1) == 0)\n",
    "    maparray = np.empty((mapsize, mapsize), dtype=np.float_)\n",
    "    maparray[0, 0] = 0\n",
    "    stepsize = mapsize\n",
    "    wibble = 100\n",
    "\n",
    "    def wibbledmean(array):\n",
    "        return array / 4 + wibble * np.random.uniform(-wibble, wibble, array.shape)\n",
    "\n",
    "    def fillsquares():\n",
    "        \"\"\"For each square of points stepsize apart,\n",
    "           calculate middle value as mean of points + wibble\"\"\"\n",
    "        cornerref = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n",
    "        squareaccum = cornerref + np.roll(cornerref, shift=-1, axis=0)\n",
    "        squareaccum += np.roll(squareaccum, shift=-1, axis=1)\n",
    "        maparray[stepsize // 2:mapsize:stepsize,\n",
    "        stepsize // 2:mapsize:stepsize] = wibbledmean(squareaccum)\n",
    "\n",
    "    def filldiamonds():\n",
    "        \"\"\"For each diamond of points stepsize apart,\n",
    "           calculate middle value as mean of points + wibble\"\"\"\n",
    "        mapsize = maparray.shape[0]\n",
    "        drgrid = maparray[stepsize // 2:mapsize:stepsize, stepsize // 2:mapsize:stepsize]\n",
    "        ulgrid = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n",
    "        ldrsum = drgrid + np.roll(drgrid, 1, axis=0)\n",
    "        lulsum = ulgrid + np.roll(ulgrid, -1, axis=1)\n",
    "        ltsum = ldrsum + lulsum\n",
    "        maparray[0:mapsize:stepsize, stepsize // 2:mapsize:stepsize] = wibbledmean(ltsum)\n",
    "        tdrsum = drgrid + np.roll(drgrid, 1, axis=1)\n",
    "        tulsum = ulgrid + np.roll(ulgrid, -1, axis=0)\n",
    "        ttsum = tdrsum + tulsum\n",
    "        maparray[stepsize // 2:mapsize:stepsize, 0:mapsize:stepsize] = wibbledmean(ttsum)\n",
    "\n",
    "    while stepsize >= 2:\n",
    "        fillsquares()\n",
    "        filldiamonds()\n",
    "        stepsize //= 2\n",
    "        wibble /= wibbledecay\n",
    "\n",
    "    maparray -= maparray.min()\n",
    "    return maparray / maparray.max()\n",
    "\n",
    "\n",
    "def clipped_zoom(img, zoom_factor):\n",
    "    h = img.shape[0]\n",
    "    # ceil crop height(= crop width)\n",
    "    ch = int(np.ceil(h / zoom_factor))\n",
    "\n",
    "    top = (h - ch) // 2\n",
    "    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n",
    "    # trim off any extra pixels\n",
    "    trim_top = (img.shape[0] - h) // 2\n",
    "\n",
    "    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n",
    "\n",
    "\n",
    "# /////////////// End Distortion Helpers ///////////////\n",
    "\n",
    "\n",
    "# /////////////// Distortions ///////////////\n",
    "\n",
    "def gaussian_noise(x, severity=1):\n",
    "    c = [.08, .12, 0.18, 0.26, 0.38][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n",
    "\n",
    "\n",
    "def shot_noise(x, severity=1):\n",
    "    c = [60, 25, 12, 5, 3][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    return np.clip(np.random.poisson(x * c) / c, 0, 1) * 255\n",
    "\n",
    "\n",
    "\n",
    "def impulse_noise(x, severity=1):\n",
    "    c = [.03, .06, .09, 0.17, 0.27][severity - 1]\n",
    "\n",
    "    x = sk.util.random_noise(np.array(x) / 255., mode='s&p', amount=c)\n",
    "    return np.clip(x, 0, 1) * 255\n",
    "\n",
    "\n",
    "def speckle_noise(x, severity=1):\n",
    "    c = [.15, .2, 0.35, 0.45, 0.6][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    return np.clip(x + x * np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n",
    "\n",
    "\n",
    "'''def fgsm(x, source_net, severity=1):\n",
    "    c = [8, 16, 32, 64, 128][severity - 1]\n",
    "\n",
    "    x = V(x, requires_grad=True)\n",
    "    logits = source_net(x)\n",
    "    source_net.zero_grad()\n",
    "    loss = F.cross_entropy(logits, V(logits.data.max(1)[1].squeeze_()), size_average=False)\n",
    "    loss.backward()\n",
    "\n",
    "    return standardize(torch.clamp(unstandardize(x.data) + c / 255. * unstandardize(torch.sign(x.grad.data)), 0, 1))'''\n",
    "\n",
    "\n",
    "def gaussian_blur(x, severity=1):\n",
    "    c = [1, 2, 3, 4, 6][severity - 1]\n",
    "\n",
    "    x = gaussian(np.array(x) / 255., sigma=c, channel_axis=-1)\n",
    "    return np.clip(x, 0, 1) * 255\n",
    "\n",
    "\n",
    "def glass_blur(x, severity=1):\n",
    "    # sigma, max_delta, iterations\n",
    "    c = [(0.7, 1, 2), (0.9, 2, 1), (1, 2, 3), (1.1, 3, 2), (1.5, 4, 2)][severity - 1]\n",
    "\n",
    "    x = np.uint8(gaussian(np.array(x) / 255., sigma=c[0], channel_axis=-1) * 255)\n",
    "\n",
    "    # locally shuffle pixels\n",
    "    for i in range(c[2]):\n",
    "        for h in range(224 - c[1], c[1], -1):\n",
    "            for w in range(224 - c[1], c[1], -1):\n",
    "                dx, dy = np.random.randint(-c[1], c[1], size=(2,))\n",
    "                h_prime, w_prime = h + dy, w + dx\n",
    "                # swap\n",
    "                x[h, w], x[h_prime, w_prime] = x[h_prime, w_prime], x[h, w]\n",
    "\n",
    "    return np.clip(gaussian(x / 255., sigma=c[0], channel_axis=-1), 0, 1) * 255\n",
    "\n",
    "\n",
    "def defocus_blur(x, severity=1):\n",
    "    c = [(3, 0.1), (4, 0.5), (6, 0.5), (8, 0.5), (10, 0.5)][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    kernel = disk(radius=c[0], alias_blur=c[1])\n",
    "\n",
    "    channels = []\n",
    "    for d in range(3):\n",
    "        channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n",
    "    channels = np.array(channels).transpose((1, 2, 0))  # 3x224x224 -> 224x224x3\n",
    "\n",
    "    return np.clip(channels, 0, 1) * 255\n",
    "\n",
    "\n",
    "def motion_blur(x, severity=1):\n",
    "    c = [(10, 3), (15, 5), (15, 8), (15, 12), (20, 15)][severity - 1]\n",
    "\n",
    "    output = BytesIO()\n",
    "    x.save(output, format='PNG')\n",
    "    x = MotionImage(blob=output.getvalue())\n",
    "\n",
    "    x.motion_blur(radius=c[0], sigma=c[1], angle=np.random.uniform(-45, 45))\n",
    "\n",
    "    x = cv2.imdecode(np.fromstring(x.make_blob(), np.uint8),\n",
    "                     cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    if x.shape != (224, 224):\n",
    "        return np.clip(x[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n",
    "    else:  # greyscale to RGB\n",
    "        return np.clip(np.array([x, x, x]).transpose((1, 2, 0)), 0, 255)\n",
    "\n",
    "\n",
    "def zoom_blur(x, severity=1):\n",
    "    c = [np.arange(1, 1.11, 0.01),\n",
    "         np.arange(1, 1.16, 0.01),\n",
    "         np.arange(1, 1.21, 0.02),\n",
    "         np.arange(1, 1.26, 0.02),\n",
    "         np.arange(1, 1.31, 0.03)][severity - 1]\n",
    "\n",
    "    x = (np.array(x) / 255.).astype(np.float32)\n",
    "    out = np.zeros_like(x)\n",
    "    for zoom_factor in c:\n",
    "        out += clipped_zoom(x, zoom_factor)\n",
    "\n",
    "    x = (x + out) / (len(c) + 1)\n",
    "    return np.clip(x, 0, 1) * 255\n",
    "\n",
    "def fog(x, severity=1):\n",
    "    c = [(1.5, 2), (2, 2), (2.5, 1.7), (2.5, 1.5), (3, 1.4)][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    max_val = x.max()\n",
    "    x += c[0] * plasma_fractal(wibbledecay=c[1])[:224, :224][..., np.newaxis]\n",
    "    return np.clip(x * max_val / (max_val + c[0]), 0, 1) * 255\n",
    "\n",
    "\n",
    "'''def frost(x, severity=1):\n",
    "    c = [(1, 0.4),\n",
    "         (0.8, 0.6),\n",
    "         (0.7, 0.7),\n",
    "         (0.65, 0.7),\n",
    "         (0.6, 0.75)][severity - 1]\n",
    "    idx = np.random.randint(5)\n",
    "    filename = ['./frost1.png', './frost2.png', './frost3.png', './frost4.jpg', './frost5.jpg', './frost6.jpg'][idx]\n",
    "    frost = cv2.imread(filename)\n",
    "    # randomly crop and convert to rgb\n",
    "    x_start, y_start = np.random.randint(0, frost.shape[0] - 224), np.random.randint(0, frost.shape[1] - 224)\n",
    "    frost = frost[x_start:x_start + 224, y_start:y_start + 224][..., [2, 1, 0]]\n",
    "\n",
    "    return np.clip(c[0] * np.array(x) + c[1] * frost, 0, 255)'''\n",
    "\n",
    "\n",
    "'''def snow(x, severity=1):\n",
    "    c = [(0.1, 0.3, 3, 0.5, 10, 4, 0.8),\n",
    "         (0.2, 0.3, 2, 0.5, 12, 4, 0.7),\n",
    "         (0.55, 0.3, 4, 0.9, 12, 8, 0.7),\n",
    "         (0.55, 0.3, 4.5, 0.85, 12, 8, 0.65),\n",
    "         (0.55, 0.3, 2.5, 0.85, 12, 12, 0.55)][severity - 1]\n",
    "\n",
    "    x = np.array(x, dtype=np.float32) / 255.\n",
    "    snow_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])  # [:2] for monochrome\n",
    "\n",
    "    snow_layer = clipped_zoom(snow_layer[..., np.newaxis], c[2])\n",
    "    snow_layer[snow_layer < c[3]] = 0\n",
    "\n",
    "    snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode='L')\n",
    "    output = BytesIO()\n",
    "    snow_layer.save(output, format='PNG')\n",
    "    snow_layer = MotionImage(blob=output.getvalue())\n",
    "\n",
    "    snow_layer.motion_blur(radius=c[4], sigma=c[5], angle=np.random.uniform(-135, -45))\n",
    "\n",
    "    snow_layer = cv2.imdecode(np.fromstring(snow_layer.make_blob(), np.uint8),\n",
    "                              cv2.IMREAD_UNCHANGED) / 255.\n",
    "    snow_layer = snow_layer[..., np.newaxis]\n",
    "\n",
    "    x = c[6] * x + (1 - c[6]) * np.maximum(x, cv2.cvtColor(x, cv2.COLOR_RGB2GRAY).reshape(224, 224, 1) * 1.5 + 0.5)\n",
    "    return np.clip(x + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255'''\n",
    "\n",
    "\n",
    "def spatter(x, severity=1):\n",
    "    c = [(0.65, 0.3, 4, 0.69, 0.6, 0),\n",
    "         (0.65, 0.3, 3, 0.68, 0.6, 0),\n",
    "         (0.65, 0.3, 2, 0.68, 0.5, 0),\n",
    "         (0.65, 0.3, 1, 0.65, 1.5, 1),\n",
    "         (0.67, 0.4, 1, 0.65, 1.5, 1)][severity - 1]\n",
    "    x = np.array(x, dtype=np.float32) / 255.\n",
    "\n",
    "    liquid_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])\n",
    "\n",
    "    liquid_layer = gaussian(liquid_layer, sigma=c[2])\n",
    "    liquid_layer[liquid_layer < c[3]] = 0\n",
    "    if c[5] == 0:\n",
    "        liquid_layer = (liquid_layer * 255).astype(np.uint8)\n",
    "        dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n",
    "        dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n",
    "        _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n",
    "        dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n",
    "        dist = cv2.equalizeHist(dist)\n",
    "        #     ker = np.array([[-1,-2,-3],[-2,0,0],[-3,0,1]], dtype=np.float32)\n",
    "        #     ker -= np.mean(ker)\n",
    "        ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n",
    "        dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n",
    "        dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n",
    "\n",
    "        m = cv2.cvtColor(liquid_layer * dist, cv2.COLOR_GRAY2BGRA)\n",
    "        m /= np.max(m, axis=(0, 1))\n",
    "        m *= c[4]\n",
    "\n",
    "        # water is pale turqouise\n",
    "        color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n",
    "                                238 / 255. * np.ones_like(m[..., :1]),\n",
    "                                238 / 255. * np.ones_like(m[..., :1])), axis=2)\n",
    "\n",
    "        color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n",
    "        x = cv2.cvtColor(x, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "        return cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255\n",
    "    else:\n",
    "        m = np.where(liquid_layer > c[3], 1, 0)\n",
    "        m = gaussian(m.astype(np.float32), sigma=c[4])\n",
    "        m[m < 0.8] = 0\n",
    "        #         m = np.abs(m) ** (1/c[4])\n",
    "\n",
    "        # mud brown\n",
    "        color = np.concatenate((63 / 255. * np.ones_like(x[..., :1]),\n",
    "                                42 / 255. * np.ones_like(x[..., :1]),\n",
    "                                20 / 255. * np.ones_like(x[..., :1])), axis=2)\n",
    "\n",
    "        color *= m[..., np.newaxis]\n",
    "        x *= (1 - m[..., np.newaxis])\n",
    "\n",
    "        return np.clip(x + color, 0, 1) * 255\n",
    "\n",
    "\n",
    "def contrast(x, severity=1):\n",
    "    c = [0.4, .3, .2, .1, .05][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    means = np.mean(x, axis=(0, 1), keepdims=True)\n",
    "    return np.clip((x - means) * c + means, 0, 1) * 255\n",
    "\n",
    "\n",
    "def brightness(x, severity=1):\n",
    "    c = [.1, .2, .3, .4, .5][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    x = sk.color.rgb2hsv(x)\n",
    "    x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n",
    "    x = sk.color.hsv2rgb(x)\n",
    "\n",
    "    return np.clip(x, 0, 1) * 255\n",
    "\n",
    "\n",
    "def saturate(x, severity=1):\n",
    "    c = [(0.3, 0), (0.1, 0), (2, 0), (5, 0.1), (20, 0.2)][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    x = sk.color.rgb2hsv(x)\n",
    "    x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n",
    "    x = sk.color.hsv2rgb(x)\n",
    "\n",
    "    return np.clip(x, 0, 1) * 255\n",
    "\n",
    "\n",
    "def jpeg_compression(x, severity=1):\n",
    "    c = [25, 18, 15, 10, 7][severity - 1]\n",
    "\n",
    "    output = BytesIO()\n",
    "    x.save(output, 'JPEG', quality=c)\n",
    "    x = PILImage.open(output)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "'''def pixelate(x, severity=1):\n",
    "    x = PILImage.fromarray(x)\n",
    "    c = [0.6, 0.5, 0.4, 0.3, 0.25][severity - 1]\n",
    "\n",
    "    x = x.resize((int(224 * c), int(224 * c)), PILImage.BOX)\n",
    "    x = x.resize((224, 224), PILImage.BOX)\n",
    "\n",
    "    x = np.array(x)\n",
    "    return x'''\n",
    "def pixelate(x, severity=1):\n",
    "    \"\"\"\n",
    "    Pixelates an image based on severity level.\n",
    "\n",
    "    Parameters:\n",
    "    x (numpy.ndarray): Input image.\n",
    "    severity (int): Severity level (1 to 5).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Pixelated image.\n",
    "    \"\"\"\n",
    "\n",
    "    # 確保輸入是 NumPy 陣列\n",
    "    if not isinstance(x, np.ndarray):\n",
    "        raise TypeError(\"Input 'x' must be a NumPy array.\")\n",
    "    \n",
    "    # 確保數據類型為 uint8\n",
    "    if x.dtype != np.uint8:\n",
    "        x = (x * 255).astype(np.uint8)\n",
    "\n",
    "    # 單通道處理（如灰階），確保轉換為 RGB\n",
    "    if x.ndim == 3 and x.shape[-1] == 1:\n",
    "        x = np.squeeze(x, axis=-1)\n",
    "        x = np.stack([x] * 3, axis=-1)  # 灰階轉換為三通道 RGB\n",
    "\n",
    "    # PIL 圖片轉換\n",
    "    x = PILImage.fromarray(x)\n",
    "    \n",
    "    # 確保 severity 在合理範圍內\n",
    "    severity = int(np.clip(severity, 1, 5))\n",
    "    c = [0.6, 0.5, 0.4, 0.3, 0.25][severity - 1]  # 對應縮放比例\n",
    "    \n",
    "    # 進行 pixelate 處理\n",
    "    x = x.resize((int(224 * c), int(224 * c)), PILImage.LANCZOS)  # 使用 LANCZOS 提高效果\n",
    "    x = x.resize((224, 224), PILImage.LANCZOS)\n",
    "\n",
    "    # 轉回 NumPy 陣列\n",
    "    x = np.array(x)\n",
    "\n",
    "    return 255 - x\n",
    "\n",
    "\n",
    "# mod of https://gist.github.com/erniejunior/601cdf56d2b424757de5\n",
    "def elastic_transform(image, severity=1):\n",
    "    c = [(244 * 2, 244 * 0.7, 244 * 0.1),   # 244 should have been 224, but ultimately nothing is incorrect\n",
    "         (244 * 2, 244 * 0.08, 244 * 0.2),\n",
    "         (244 * 0.05, 244 * 0.01, 244 * 0.02),\n",
    "         (244 * 0.07, 244 * 0.01, 244 * 0.02),\n",
    "         (244 * 0.12, 244 * 0.01, 244 * 0.02)][severity - 1]\n",
    "\n",
    "    image = np.array(image, dtype=np.float32) / 255.\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "\n",
    "    # random affine\n",
    "    center_square = np.float32(shape_size) // 2\n",
    "    square_size = min(shape_size) // 3\n",
    "    pts1 = np.float32([center_square + square_size,\n",
    "                       [center_square[0] + square_size, center_square[1] - square_size],\n",
    "                       center_square - square_size])\n",
    "    pts2 = pts1 + np.random.uniform(-c[2], c[2], size=pts1.shape).astype(np.float32)\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    dx = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n",
    "                   c[1], mode='reflect', truncate=3) * c[0]).astype(np.float32)\n",
    "    dy = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n",
    "                   c[1], mode='reflect', truncate=3) * c[0]).astype(np.float32)\n",
    "    dx, dy = dx[..., np.newaxis], dy[..., np.newaxis]\n",
    "\n",
    "    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    "    return np.clip(map_coordinates(image, indices, order=1, mode='reflect').reshape(shape), 0, 1) * 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################\n",
    "def reverse_normalize(image, mean, std):\n",
    "    # 確保均值和標準差是 numpy array，以支持逐像素計算\n",
    "    mean = np.array(mean).reshape(1, 1, -1)  # [1, 1, C]\n",
    "    std = np.array(std).reshape(1, 1, -1)    # [1, 1, C]\n",
    "\n",
    "    # 反標準化公式\n",
    "    image = (image * std) + mean\n",
    "    return np.clip(image, 0, 255)  # 確保範圍在 [0, 255]\n",
    "\n",
    "def normalize(image, mean, std):\n",
    "    # 將影像標準化\n",
    "    mean = np.array(mean).reshape(1, 1, -1)  # [1, 1, C]\n",
    "    std = np.array(std).reshape(1, 1, -1)    # [1, 1, C]\n",
    "    image = (image - mean) / std\n",
    "    return image\n",
    "\n",
    "# Custom dataset to apply elastic_transform\n",
    "# 自定義數據集\n",
    "class IndexBasedTransformDataset(Dataset):\n",
    "    def __init__(self, original_dataset, transforms_by_range, repeat_factor=1):\n",
    "        \"\"\"\n",
    "        original_dataset: 原始 PyTorch Dataset\n",
    "        transforms_by_range: 列表，每個元素是 (start_index, end_index, transform_function)\n",
    "        repeat_factor: 數據集重複的倍數\n",
    "        \"\"\"\n",
    "        self.original_dataset = original_dataset\n",
    "        self.transforms_by_range = transforms_by_range\n",
    "        self.repeat_factor = repeat_factor  # 數據集重複幾次\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset) * self.repeat_factor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 對數據索引進行取模，將重複部分對應回原始數據\n",
    "        original_idx = idx % len(self.original_dataset)\n",
    "        image, label = self.original_dataset[original_idx]\n",
    "        \n",
    "        # 轉換影像形狀為 [H, W, C]\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.permute(1, 2, 0).numpy()  # 從 [C, H, W] 變為 [H, W, C]\n",
    "        \n",
    "        # Step 1: 反標準化\n",
    "        image = reverse_normalize(image, mean=[103.939, 116.779, 123.68], std=[1.0, 1.0, 1.0])\n",
    "\n",
    "        # Step 2: 影像增強\n",
    "        for start, end, transform in self.transforms_by_range:\n",
    "            if start <= idx < end:\n",
    "                if transform is not None:\n",
    "                    image = transform(image)\n",
    "                break\n",
    "        \n",
    "        # Step 3: 標準化\n",
    "        image = normalize(image, mean=[103.939, 116.779, 123.68], std=[1.0, 1.0, 1.0])\n",
    "\n",
    "        # 還原影像形狀為 [C, H, W]\n",
    "        image = np.transpose(image, (2, 0, 1))  # 從 [H, W, C] 變回 [C, H, W]\n",
    "        return image, label\n",
    "    \n",
    "test_dataset, _ = random_split(dataset, [limit, len(dataset) - limit])\n",
    "image, _ = test_dataset[0]\n",
    "print(image.shape)\n",
    "'''corruption_types = ['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', \n",
    "                    'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', \n",
    "                    'pixelate', 'saturate', 'shot_noise', 'spatter', 'speckle_noise', 'zoom_blur']'''\n",
    "\n",
    "severity = 5\n",
    "# 為每 1000 筆設置對應的增強方法\n",
    "transforms_by_range = [\n",
    "    (0, limit, lambda img: brightness(img, severity=severity)),\n",
    "    (limit, 2 * limit, lambda img: contrast(img, severity=severity)),\n",
    "    (2 * limit, 3 * limit, lambda img: defocus_blur(img, severity=severity)),\n",
    "    (3 * limit, 4 * limit, lambda img: elastic_transform(img, severity=severity)),\n",
    "    (4 * limit, 5 * limit, lambda img: fog(img, severity=severity)),\n",
    "    (5 * limit, 6 * limit, lambda img: gaussian_blur(img, severity=severity)),\n",
    "    (6 * limit, 7 * limit, lambda img: gaussian_noise(img, severity=severity)),\n",
    "    (7 * limit, 8 * limit, lambda img: glass_blur(img, severity=severity)),\n",
    "    (8 * limit, 9 * limit, lambda img: impulse_noise(img, severity=severity)),\n",
    "    (9 * limit, 10 * limit, lambda img: pixelate(img, severity=severity)), ## need check\n",
    "    (10 * limit, 11 * limit, lambda img: saturate(img, severity=severity)),\n",
    "    (11 * limit, 12 * limit, lambda img: shot_noise(img, severity=severity)),\n",
    "    (12 * limit, 13 * limit, lambda img: spatter(img, severity=severity)),\n",
    "    (13 * limit, 14 * limit, lambda img: speckle_noise(img, severity=severity)),\n",
    "    (14 * limit, 15 * limit, lambda img: zoom_blur(img, severity=severity)),\n",
    "    #(15 * limit, 16 * limit, None),\n",
    "]\n",
    "\n",
    "test_dataset = IndexBasedTransformDataset(test_dataset, transforms_by_range, len(transforms_by_range))\n",
    "\n",
    "# 創建 DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=6, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_vanilla = []\n",
    "# 預測並顯示分數\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "img, _ = test_dataset[0]\n",
    "print(len(test_dataset))\n",
    "\n",
    "\n",
    "'''fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "class_names = {0:'brightness', 1:'contrast', 2:'defocus_blur', 3:'elastic_transform', 4:'fog', \n",
    "                    5:'gaussian_blur', 6:'gaussian_noise', 7:'glass_blur', 8:'impulse_noise', \n",
    "                    9:'pixelate', 10:'saturate', 11:'shot_noise', 12:'spatter', 13:'speckle_noise', 14:'zoom_blur'}  # 你的類別名稱\n",
    "for i in range(15):\n",
    "    index = limit * i  # 計算索引\n",
    "    img, _ = test_dataset[index]  # 讀取影像\n",
    "\n",
    "    # 反正規化影像\n",
    "    img = reverse_normalize(img.transpose(1, 2, 0), mean=[103.939, 116.779, 123.68], std=[1.0, 1.0, 1.0]) / 255\n",
    "    \n",
    "    ax = axes[i // 5, i % 5]  # 計算子圖位置\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{class_names[i]}\", fontsize=12)\n",
    "    ax.axis(\"off\")  # 隱藏座標軸\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 計算Softmax並獲取最大值\n",
    "        softmax_probs = F.softmax(outputs, dim=1)  # Softmax值\n",
    "        max_probs, predicted = torch.max(softmax_probs, 1)\n",
    "\n",
    "        conf_vanilla.extend(max_probs.cpu().numpy()) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        #print(model)\n",
    "        #print(labels, predicted)\n",
    "        \n",
    "conf_vanilla = np.array(conf_vanilla)\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bzq modifying\n",
    "len_x = len_x_target\n",
    "len_y = len_y_target\n",
    "stride_x = stride_x_target\n",
    "stride_y = stride_y_target\n",
    "batch_size = 1  # 設定批次大小\n",
    "\n",
    "original_predictions_imagenet = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        batch_data = batch_data.to(device).float()\n",
    "        batch_predictions_imagenet = model(batch_data).cpu().numpy()\n",
    "        original_predictions_imagenet.append(batch_predictions_imagenet)\n",
    "\n",
    "original_predictions_imagenet = np.vstack(original_predictions_imagenet)\n",
    "\n",
    "# 使用正確的方式來訪問 test_dataset 中的標籤\n",
    "test_labels = [test_dataset[i][1] for i in range(len(test_dataset))]\n",
    "\n",
    "predicted_labels = np.argmax(original_predictions_imagenet, axis=1)\n",
    "correct_indices = np.where(predicted_labels == np.array(test_labels))[0]\n",
    "incorrect_indices = np.where(predicted_labels != np.array(test_labels))[0]\n",
    "\n",
    "correct_predictions_imagenet.extend(correct_indices.tolist())\n",
    "incorrect_predictions_imagenet.extend(incorrect_indices.tolist())\n",
    "\n",
    "print(f\"{len(correct_predictions_imagenet)}, {len(incorrect_predictions_imagenet)}\")\n",
    "\n",
    "'''original_predictions_imagenet = []\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        batch_predictions_imagenet = model(batch_data)\n",
    "        original_predictions_imagenet.append(batch_predictions_imagenet.cpu().numpy())\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    if np.argmax(original_predictions_imagenet[i]) == test_dataset[i][1]:\n",
    "        correct_predictions_imagenet.append(i)\n",
    "    else:\n",
    "        incorrect_predictions_imagenet.append(i)\n",
    "\n",
    "original_predictions_imagenet = np.vstack(original_predictions_imagenet)\n",
    "\n",
    "print(f\"{len(correct_predictions_imagenet)}, {len(incorrect_predictions_imagenet)}\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "random_num_for_bzq_mask_imagenet = np.random.randint(0, 256, (3, len_y_target, len_x_target)).astype(np.float32) / 255.0\n",
    "\n",
    "def brier_score_decomposition(predictions, labels, num_classes=365):\n",
    "    \"\"\"\n",
    "    計算 Brier Score 並拆解為 Uncertainty, Resolution, Reliability\n",
    "    predictions: (batch_size, num_classes) - softmax 輸出\n",
    "    labels: (batch_size) - 正確標籤\n",
    "    \"\"\"\n",
    "    labels_one_hot = torch.nn.functional.one_hot(labels, num_classes).float()\n",
    "\n",
    "    # 計算 Brier Score\n",
    "    brier_score = torch.mean(torch.sum((predictions - labels_one_hot) ** 2, dim=1))\n",
    "\n",
    "    # 計算 Uncertainty\n",
    "    marginal_probs = torch.mean(predictions, dim=0)\n",
    "    uncertainty = torch.sum(marginal_probs * (1 - marginal_probs))\n",
    "\n",
    "    # 計算 Resolution\n",
    "    mean_class_probs = torch.mean(labels_one_hot, dim=0)\n",
    "    resolution = torch.sum(mean_class_probs * (1 - mean_class_probs))\n",
    "\n",
    "    # 計算 Reliability\n",
    "    reliability = brier_score - uncertainty + resolution  # 修正計算方式\n",
    "\n",
    "    return {\n",
    "        \"Brier Score\": brier_score.item(),\n",
    "        \"Uncertainty\": uncertainty.item(),\n",
    "        \"Resolution\": resolution.item(),\n",
    "        \"Reliability\": reliability.item()\n",
    "    }\n",
    "\n",
    "def compute_nll(predictions, labels):\n",
    "    \"\"\"\n",
    "    計算 Negative Log-Likelihood (NLL)\n",
    "    predictions: shape (batch_size, num_classes) - softmax 輸出\n",
    "    labels: shape (batch_size) - 正確的標籤\n",
    "    \"\"\"\n",
    "    log_probs = torch.log(torch.clamp(predictions, min=1e-9))  # 避免 log(0) 問題\n",
    "    labels = labels.expand(predictions.size(0))\n",
    "    nll_loss = F.nll_loss(log_probs, labels)\n",
    "    return nll_loss.item()\n",
    "\n",
    "def process_batch(batch_data, batch_labels, len_y, stride_y, len_x, stride_x, device, model, alpha, bzq):\n",
    "    if len(batch_data) == 0:\n",
    "        return bzq, np.array([])\n",
    "\n",
    "    targets = []\n",
    "\n",
    "    for i in range(0, 224 - len_y, stride_y):\n",
    "        for j in range(0, 224 - len_x, stride_x):\n",
    "            #target = single_data_bzq_mask_preprocessing_imagenet(batch_data, i, j, len_x, len_y, 0)\n",
    "            target = single_data_bzq_mask_preprocessing_imagenet_random_global(batch_data, i, j, len_x, len_y, random_num_for_bzq_mask_imagenet)\n",
    "            targets.append(target)\n",
    "    \n",
    "    targets_tensor = torch.from_numpy(np.vstack(targets).reshape(-1, 3, 224, 224)).float().to(device)\n",
    "    predictions = model(targets_tensor)\n",
    "    \n",
    "    #temp, _ = torch.max(F.softmax(predictions), dim=1)\n",
    "    #temp = temp.cpu().numpy()\n",
    "    \n",
    "    max_bzq_indices = torch.argmax(predictions, dim=1).cpu().numpy()\n",
    "    \n",
    "    # 計算 softmax\n",
    "    softmax_predictions = F.softmax(predictions, dim=1)\n",
    "\n",
    "    labels = batch_labels\n",
    "\n",
    "    # 計算 Brier Score 分解與 NLL\n",
    "    brier_components = brier_score_decomposition(softmax_predictions, labels)\n",
    "    nll_loss = compute_nll(softmax_predictions, labels)\n",
    "\n",
    "    counter = Counter(max_bzq_indices)\n",
    "    most_common_num, most_common_count = counter.most_common(1)[0]\n",
    "    \n",
    "    temp = softmax_predictions[:, most_common_num].cpu().numpy()\n",
    "    \n",
    "    bzq.append(alpha * np.mean(temp) + (1 - alpha) * (2.0 / np.pi * np.arctan(1.0 / np.std(temp))))\n",
    "\n",
    "    return bzq, temp, brier_components, nll_loss\n",
    "\n",
    "# 初始化儲存 Brier Score 分解部分與 NLL 的陣列\n",
    "brier_scores = []\n",
    "uncertainties = []\n",
    "resolutions = []\n",
    "reliabilities = []\n",
    "nll_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        batch_data = batch_data.to(device)  # 將批次影像搬到指定裝置\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        if len(batch_data) > 0:\n",
    "            bzq, temp, brier_components, nll_loss = process_batch(\n",
    "                batch_data, batch_labels, len_y, stride_y, len_x, stride_x, device, model, alpha, bzq\n",
    "            )\n",
    "\n",
    "            brier_scores.append(brier_components[\"Brier Score\"])\n",
    "            uncertainties.append(brier_components[\"Uncertainty\"])\n",
    "            resolutions.append(brier_components[\"Resolution\"])\n",
    "            reliabilities.append(brier_components[\"Reliability\"])\n",
    "            nll_losses.append(nll_loss)\n",
    "\n",
    "            # 影像預處理\n",
    "            original_data = single_data_bzq_mask_preprocessing_imagenet(batch_data, 0, 0, 0, 0, 0)\n",
    "            original_prediction = model(torch.tensor(original_data.reshape(-1, 3, 224, 224)).float().to(device))\n",
    "            max_original_index = torch.argmax(original_prediction).item()\n",
    "\n",
    "            # 根據 `bzq` 值分類統計\n",
    "            if bzq[-1] == 0.0:\n",
    "                if (len(bzq) - 1) in correct_predictions_imagenet:\n",
    "                    correct_predictions_bzq_zero_softmax_mean.append(np.mean(temp))\n",
    "                    correct_predictions_bzq_zero_softmax_std.append(np.std(temp))\n",
    "                else:\n",
    "                    incorrect_predictions_bzq_zero_softmax_mean.append(np.mean(temp))\n",
    "                    incorrect_predictions_bzq_zero_softmax_std.append(np.std(temp))\n",
    "\n",
    "bzq_imagenet = np.array(bzq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_bzq_imagenet = 1 / bzq_imagenet\n",
    "result_bzq_imagenet = bzq_imagenet        \n",
    "\n",
    "\n",
    "counts, bins, patches = plt.hist(bzq_imagenet, bins=bins_size)\n",
    "'''plt.title('Cumulative Histogram of Correct Predictions')\n",
    "plt.xlabel('bzq')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')  # 指定圖例位置\n",
    "plt.show()\n",
    "\n",
    "# 打印結果\n",
    "plt.boxplot(bzq_imagenet)\n",
    "plt.show()'''\n",
    "\n",
    "bzq_correct_imagenet = np.array([bzq_imagenet[i] for i in correct_predictions_imagenet])\n",
    "bzq_incorrect_imagenet = np.array([bzq_imagenet[i] for i in incorrect_predictions_imagenet])\n",
    "\n",
    "result_bzq_correct_imagenet = np.array([result_bzq_imagenet[i] for i in correct_predictions_imagenet])\n",
    "result_bzq_incorrect_imagenet = np.array([result_bzq_imagenet[i] for i in incorrect_predictions_imagenet])\n",
    "\n",
    "# 繪製分布圖\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.histplot(correct_predictions_bzq_zero_softmax_mean, bins=15) \n",
    "plt.show()\n",
    "# 繪製分布圖\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.histplot(incorrect_predictions_bzq_zero_softmax_mean, bins=15)\n",
    "plt.show()\n",
    "# 繪製分布圖\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.histplot(correct_predictions_bzq_zero_softmax_std, bins=15) \n",
    "plt.show()\n",
    "# 繪製分布圖\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.histplot(incorrect_predictions_bzq_zero_softmax_std, bins=15) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate correct and incorrect predictions\n",
    "bzq_correct_imagenet = np.array([bzq_imagenet[i] for i in correct_predictions_imagenet])\n",
    "bzq_incorrect_imagenet = np.array([bzq_imagenet[i] for i in incorrect_predictions_imagenet])\n",
    "\n",
    "result_bzq_correct_imagenet = np.array([result_bzq_imagenet[i] for i in correct_predictions_imagenet])\n",
    "result_bzq_incorrect_imagenet = np.array([result_bzq_imagenet[i] for i in incorrect_predictions_imagenet])\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Cumulative Histogram of Correct Predictions\n",
    "axs[0, 0].hist(bzq_correct_imagenet, bins=bins_size)\n",
    "axs[0, 0].set_title('Cumulative Histogram of Correct Predictions')\n",
    "axs[0, 0].set_xlabel('bzq')\n",
    "axs[0, 0].set_ylabel('Count')\n",
    "axs[0, 0].legend(['Correct Predictions'], loc='upper right')\n",
    "\n",
    "# Boxplot of bzq_imagenet\n",
    "axs[0, 1].boxplot(bzq_correct_imagenet)\n",
    "axs[0, 1].set_title('Boxplot of bzq_imagenet')\n",
    "\n",
    "# Cumulative Histogram of Correct Predictions\n",
    "axs[1, 0].hist(bzq_incorrect_imagenet, bins=bins_size)\n",
    "axs[1, 0].set_title('Cumulative Histogram of Incorrect Predictions')\n",
    "axs[1, 0].set_xlabel('bzq')\n",
    "axs[1, 0].set_ylabel('Count')\n",
    "axs[1, 0].legend(['Incorrect Predictions'], loc='upper right')\n",
    "\n",
    "# Boxplot of bzq_correct_imagenet\n",
    "axs[1, 1].boxplot(bzq_incorrect_imagenet)\n",
    "axs[1, 1].set_title('Boxplot of bzq_incorrect_imagenet')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 劃出confidence-acc 圖: confidence由bzq提供，acc由該confidence數值底下預測準確的\n",
    "\n",
    "result_pred_imagenet = np.ones(len(test_dataset)) \n",
    "for i in incorrect_predictions_imagenet:\n",
    "    result_pred_imagenet[i] = 0\n",
    "\n",
    "print(sum(result_pred_imagenet))\n",
    "\n",
    "result_imagenet_dict = {}\n",
    "for i, val in enumerate(result_bzq_imagenet):\n",
    "    if val not in result_imagenet_dict.keys():\n",
    "        result_imagenet_dict[val] = [result_pred_imagenet[i]]\n",
    "    else:\n",
    "        result_imagenet_dict[val].append(result_pred_imagenet[i])\n",
    "\n",
    "# 初始化信心值和準確率列表\n",
    "confidence_values = []\n",
    "accuracies = []\n",
    "element_counts = []\n",
    "\n",
    "# 計算每個信心值範圍的準確率\n",
    "for confidence in sorted(result_imagenet_dict.keys(), reverse=True):\n",
    "    combined_results = []\n",
    "    for key in result_imagenet_dict:\n",
    "        if key >= confidence:\n",
    "            combined_results.extend(result_imagenet_dict[key])\n",
    "    element_count = len(combined_results)\n",
    "    accuracy = np.mean(combined_results)\n",
    "    confidence_values.append(confidence)\n",
    "    accuracies.append(accuracy)\n",
    "    element_counts.append(element_count)\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy (Rotated 60°)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values, element_counts, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Number of Elements (p(y|x) >= τ)')\n",
    "plt.title('Confidence Threshold vs Number of Elements')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "confidence_values_scaled = np.array(confidence_values)\n",
    "#confidence_values_scaled = 2 / np.pi * np.arctan(confidence_values_scaled)\n",
    "#confidence_values_scaled = confidence_values_scaled * confidence_values_scaled / (1 - confidence_values_scaled * confidence_values_scaled)\n",
    "                                                                                  \n",
    "\n",
    "#print(confidence_values_scaled)\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_scaled, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy (Rotated 60°)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "confidence_values_scaled = scaler.fit_transform(np.array(confidence_values_scaled).reshape(-1, 1)).flatten()\n",
    "#print(confidence_values_scaled)\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_scaled, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy ')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_scaled, element_counts, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "#vanilla\n",
    "#original_predictions_imagenet (800000, 10)\n",
    "# 初始化信心值和準確率列表\n",
    "\n",
    "# 初始化 confidence_map_vanilla 為 defaultdict\n",
    "confidence_map_vanilla = defaultdict(list)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "# 將預測結果和信心值存入字典\n",
    "for i, val in enumerate(original_predictions_imagenet):\n",
    "    conf = np.max(softmax(val))\n",
    "    confidence_map_vanilla[conf].append(result_pred_imagenet[i])\n",
    "\n",
    "print(\"finish\")\n",
    "print(len(confidence_map_vanilla))\n",
    "\n",
    "confidence_values_vanilla = []\n",
    "accuracies_vanilla = []\n",
    "element_counts_vanilla = []\n",
    "\n",
    "# 計算每個信心值範圍的準確率\n",
    "sorted_confidences = sorted(confidence_map_vanilla.keys(), reverse=True)\n",
    "combined_results_vanilla = []\n",
    "\n",
    "for confidence in sorted_confidences:\n",
    "    combined_results_vanilla.extend(confidence_map_vanilla[confidence])\n",
    "    element_count_vanilla = len(combined_results_vanilla)\n",
    "    accuracy_vanilla = np.mean(combined_results_vanilla)\n",
    "    confidence_values_vanilla.append(confidence)\n",
    "    accuracies_vanilla.append(accuracy_vanilla)\n",
    "    element_counts_vanilla.append(element_count_vanilla)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_vanilla, accuracies_vanilla, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()'''\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_scaled, element_counts, marker='.', linestyle='-', label='Scaled', markersize=4)\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Number of Elements (p(y|x) >= τ)')\n",
    "plt.title('Confidence Threshold vs Number of Elements')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_vanilla, accuracies_vanilla, marker='o', linestyle='-', color='b', label='Vanilla')\n",
    "plt.plot(confidence_values_scaled, accuracies, marker='o', linestyle='-', color='r', label='Scaled')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()'''\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_scaled, accuracies, marker='.', linestyle='-', label='Scaled', markersize=4)\n",
    "\n",
    "# 新增垂直線\n",
    "plt.axvline(x=0.6827, color='g', linestyle='--', label='x=0.6827')\n",
    "plt.axvline(x=0.9545, color='m', linestyle='--', label='x=0.9545')\n",
    "plt.axvline(x=0.9973, color='c', linestyle='--', label='x=0.9973')\n",
    "\n",
    "# 找到最接近的值\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "idx_6827_scaled = find_nearest(confidence_values_scaled, 0.6827)\n",
    "idx_9545_scaled = find_nearest(confidence_values_scaled, 0.9545)\n",
    "idx_9973_scaled = find_nearest(confidence_values_scaled, 0.9973)\n",
    "\n",
    "\n",
    "plt.scatter([confidence_values_scaled[idx_6827_scaled], confidence_values_scaled[idx_9545_scaled], confidence_values_scaled[idx_9973_scaled]], \n",
    "            [accuracies[idx_6827_scaled], accuracies[idx_9545_scaled], accuracies[idx_9973_scaled]], \n",
    "            zorder=5)\n",
    "\n",
    "plt.text(confidence_values_scaled[idx_6827_scaled], accuracies[idx_6827_scaled], f'({confidence_values_scaled[idx_6827_scaled]:.3f}, {accuracies[idx_6827_scaled]:.3f})', fontsize=14, ha='right') \n",
    "plt.text(confidence_values_scaled[idx_9545_scaled], accuracies[idx_9545_scaled], f'({confidence_values_scaled[idx_9545_scaled]:.3f}, {accuracies[idx_9545_scaled]:.3f})', fontsize=14, ha='right') \n",
    "plt.text(confidence_values_scaled[idx_9973_scaled], accuracies[idx_9973_scaled], f'({confidence_values_scaled[idx_9973_scaled]:.3f}, {accuracies[idx_9973_scaled]:.3f})', fontsize=14, ha='right')\n",
    "\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(accuracies[idx_6827_scaled], accuracies[idx_9545_scaled], accuracies[idx_9973_scaled])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存儲到 .npy 檔案 \n",
    "print(random_num_for_bzq_mask_imagenet)\n",
    "'''np.save('confidence_values_vanilla.npy', confidence_values_vanilla) \n",
    "np.save('accuracies_vanilla.npy', accuracies_vanilla) \n",
    "np.save('element_counts_vanilla.npy', element_counts_vanilla)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bzq_imagenet_modified = scaler.fit_transform(np.array(result_bzq_imagenet).reshape(-1, 1)).flatten()\n",
    "result_bzq_imagenet_modified = result_bzq_imagenet\n",
    "print(np.sum([item for sublist in confidence_map_vanilla.values() for item in sublist]))\n",
    "#ECE calc\n",
    "\n",
    "def calculate_ece(confidences, labels, num_bins=15):\n",
    "    bin_boundaries = np.linspace(0, 1, num_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = np.logical_and(confidences > bin_lower, confidences <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(labels[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "    return ece\n",
    "\n",
    "\n",
    "print(result_bzq_imagenet)\n",
    "# 計算ECE\n",
    "ece = [calculate_ece(result_bzq_imagenet_modified[limit * i: limit * (i + 1)], result_pred_imagenet[limit * i: limit * (i + 1)]) for i in range(15)]\n",
    "\n",
    "ece_vanilla = [calculate_ece(conf_vanilla[limit * i: limit * (i + 1)], result_pred_imagenet[limit * i: limit * (i + 1)]) for i in range(15)]\n",
    "\n",
    "print(\"Expected Calibration Error (ECE):\", ece)\n",
    "fig, ax = plt.subplots() \n",
    "ax.boxplot(ece) \n",
    "ax.set_title('ECE Boxplot') \n",
    "ax.set_ybound(0, 0.7)\n",
    "ax.set_ylabel('ECE') \n",
    "plt.show()\n",
    "\n",
    "print(\"Expected Calibration Error (ECE):\", ece_vanilla)\n",
    "fig, ax = plt.subplots() \n",
    "ax.boxplot(ece_vanilla) \n",
    "ax.set_title('ECE Boxplot') \n",
    "ax.set_ybound(0, 0.7)\n",
    "ax.set_ylabel('ECE') \n",
    "plt.show()\n",
    "print(ece, ece_vanilla)\n",
    "print(np.percentile(ece, 25), np.percentile(ece, 50), np.percentile(ece, 75))\n",
    "print(np.percentile(ece_vanilla, 25), np.percentile(ece_vanilla, 50), np.percentile(ece_vanilla, 75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ece_modified)\n",
    "ece = [calculate_ece(result_bzq_imagenet_modified[limit * i: limit * (i + 1)], result_pred_imagenet[limit * i: limit * (i + 1)]) for i in range(15)]\n",
    "nll = [np.mean(nll_losses[limit * i : limit * (i + 1)]) for i in range(len(nll_losses) // limit)]\n",
    "bs = [np.mean(brier_scores[limit * i : limit * (i + 1)]) for i in range(len(brier_scores) // limit)]\n",
    "top5_ece = sorted(ece, reverse=False)[:5]\n",
    "print(ece)\n",
    "print(nll)\n",
    "print(bs)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(ece)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"ECE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(top5_ece)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"ECE top 5\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(nll)\n",
    "plt.ylim(0, 12)\n",
    "plt.title(\"NLL\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(bs)\n",
    "plt.ylim(0, 1.6)\n",
    "plt.title(\"Brier Score\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
