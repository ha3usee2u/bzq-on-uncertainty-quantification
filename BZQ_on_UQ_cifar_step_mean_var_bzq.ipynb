{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bzq modifying\n",
    "len_x_target = 3\n",
    "len_y_target = 3\n",
    "stride_x_target = 1\n",
    "stride_y_target = 1\n",
    "\n",
    "# mean, std proportion\n",
    "alpha = 0.5\n",
    "\n",
    "bins_size = 30  # 統計採樣數\n",
    "poly_degree = bins_size - 1\n",
    "window_size = 1\n",
    "\n",
    "#target image preprocessing\n",
    "angle = 0\n",
    "pixels = 0\n",
    "\n",
    "limit = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.interpolate import interp1d\n",
    "import tensorflow_datasets as tfds\n",
    "from collections import Counter\n",
    "import scipy.ndimage\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "import gc\n",
    "import random\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet-20 V1 architecture\n",
    "def resnet_block(inputs, filters, kernel_size=3, stride=1, activation='relu'):\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if stride != 1 or inputs.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=1, strides=stride, padding='same')(inputs)\n",
    "    else:\n",
    "        shortcut = inputs\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def resnet_v1(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(16, kernel_size=3, strides=1, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    for _ in range(3):\n",
    "        x = resnet_block(x, 16)\n",
    "    for _ in range(3):\n",
    "        x = resnet_block(x, 32, stride=2)\n",
    "    for _ in range(3):\n",
    "        x = resnet_block(x, 64, stride=2)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    #x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    model = models.Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "'''def custom_preprocessing(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    #image = tf.pad(image, [[4, 4], [4, 4], [0, 0]])\n",
    "    #image = tf.image.random_crop(image, (32, 32, 3))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image, label'''\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "y_train, y_test = tf.keras.utils.to_categorical(y_train), tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# 創建 tf.data.Dataset 並加入 RandomRotation \n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.Lambda(lambda x: tf.image.random_flip_left_right(x)),\n",
    "    #layers.Lambda(lambda x: tf.pad(x, [[4, 4], [4, 4], [0, 0]])), \n",
    "    #layers.Lambda(lambda x: tf.image.random_crop(x, (32, 32, 3))), \n",
    "    layers.Lambda(lambda x: tf.cast(x, tf.float32)),\n",
    "    #layers.RandomRotation(1)\n",
    "    ]) # 隨機旋轉圖片\n",
    "                                         \n",
    "# Create tf.data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#train_dataset = train_dataset.map(custom_preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=50000).batch(7).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.map(lambda x, y: (tf.cast(x, tf.float32), y)).batch(7)\n",
    "\n",
    "# Define model\n",
    "model = resnet_v1(input_shape=(32, 32, 3), num_classes=10)\n",
    "\n",
    "# Compile model\n",
    "initial_lr = 0.000717\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=initial_lr),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "# Learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    lr = initial_lr\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Train model\n",
    "epochs = 200\n",
    "if os.path.exists('cifarc.weights.h5'):\n",
    "    model.load_weights(\"cifarc.weights.h5\")\n",
    "    model.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "else:\n",
    "    model.fit(train_dataset,\n",
    "              validation_data=test_dataset,\n",
    "              epochs=epochs,\n",
    "              callbacks=[lr_scheduler])\n",
    "    model.save_weights(\"cifarc.weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到了bzq 正確的函數，拿來做cifar 正確的預測\n",
    "@jit\n",
    "def single_data_bzq_mask_preprocessing_cifar(original_data, start_x, start_y, len_x, len_y, magnification):\n",
    "    if len_x <= 0 or len_y <= 0:\n",
    "        return original_data\n",
    "    new_data = np.copy(original_data)\n",
    "    new_data[start_y:start_y + len_y, start_x:start_x + len_x, :] *= magnification\n",
    "    return new_data\n",
    "\n",
    "\n",
    "#print(random_num_for_bzq_mask_cifar)\n",
    "@jit\n",
    "def single_data_bzq_mask_preprocessing_cifar_random_global(original_data, start_x, start_y, len_x, len_y, random_num_for_bzq_mask_cifar):\n",
    "    if len_x <= 0 or len_y <= 0:\n",
    "        return original_data\n",
    "    new_data = np.copy(original_data)\n",
    "    random_num_for_bzq_mask_cifar = random_num_for_bzq_mask_cifar[:len_y, :len_x, :] \n",
    "    new_data[start_y:start_y + len_y, start_x:start_x + len_x, :] = random_num_for_bzq_mask_cifar\n",
    "    return new_data\n",
    "\n",
    "#bzq = []\n",
    "correct_predictions_cifar = []\n",
    "incorrect_predictions_cifar = []\n",
    "bzq_cifar = []\n",
    "\n",
    "# 下載並準備CIFAR-10-C資料集\n",
    "def load_cifar10_c():\n",
    "    url = 'https://zenodo.org/record/2535967/files/CIFAR-10-C.tar'\n",
    "    path = tf.keras.utils.get_file('CIFAR-10-C.tar', url, untar=True)\n",
    "    return path\n",
    "\n",
    "# 載入CIFAR-10-C資料集\n",
    "def load_cifar10_c_data(data_dir):\n",
    "    #corruption_types = ['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'jpeg_compression', 'motion_blur', 'pixelate', 'saturate', 'shot_noise', 'snow', 'spatter', 'speckle_noise', 'zoom_blur']\n",
    "    \n",
    "    corruption_types = ['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'pixelate', 'saturate', 'shot_noise', 'spatter', 'speckle_noise', 'zoom_blur']\n",
    "    #corruption_types = ['gaussian_blur']\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for corruption in corruption_types:\n",
    "        print(corruption)\n",
    "        file_path = os.path.join(data_dir.replace(\".tar\", \"\"), f'{corruption}.npy')\n",
    "        with open(file_path, 'rb') as f:\n",
    "            all_images = np.load(f)\n",
    "            # 調整切片範圍\n",
    "            selected_images = np.concatenate([\n",
    "                all_images[0:limit], \n",
    "                all_images[10000:10000+limit], \n",
    "                all_images[20000:20000+limit], \n",
    "                all_images[30000:30000+limit], \n",
    "                all_images[40000:40000+limit]\n",
    "            ])\n",
    "            #selected_images = all_images[0:limit]\n",
    "            images.append(selected_images)\n",
    "            #images.append(np.load(f))\n",
    "            \n",
    "         # 同樣方式處理 labels.npy\n",
    "        labels_file = os.path.join(data_dir.replace(\".tar\", \"\"), 'labels.npy')\n",
    "        with open(labels_file, 'rb') as f:\n",
    "            all_labels = np.load(f)\n",
    "\n",
    "            selected_labels = np.concatenate([\n",
    "                all_labels[0:limit], \n",
    "                all_labels[10000:10000+limit], \n",
    "                all_labels[20000:20000+limit], \n",
    "                all_labels[30000:30000+limit], \n",
    "                all_labels[40000:40000+limit]\n",
    "            ])\n",
    "            #selected_labels = all_labels[0:limit]\n",
    "            labels.append(selected_labels)\n",
    "\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return images, labels\n",
    "\n",
    "# 下載資料集\n",
    "cifar10_c_path = load_cifar10_c()\n",
    "\n",
    "\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "y_train, y_test = tf.keras.utils.to_categorical(y_train), tf.keras.utils.to_categorical(y_test)\n",
    "test_images = np.float32(x_test)\n",
    "test_labels = y_test\n",
    "preprocessed_data = test_images\n",
    "\n",
    "'''\n",
    "#載入CIFAR-10-C資料集\n",
    "test_images, test_labels = load_cifar10_c_data(cifar10_c_path)\n",
    "\n",
    "test_images = np.float32(test_images)\n",
    "preprocessed_data = test_images\n",
    "\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_data.shape, test_labels.shape)\n",
    "\n",
    "def data_generator(preprocessed_data, test_labels):\n",
    "    for image, label in zip(preprocessed_data, test_labels):\n",
    "        yield image, label\n",
    "\n",
    "batch_size = 32  # 可以根據需要調整批次大小\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(preprocessed_data, test_labels),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(32, 32, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(10,), dtype=tf.int64)\n",
    "    )\n",
    ")\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "scores = model.evaluate(dataset)\n",
    "print(f\"Test Loss: {scores[0]}\")\n",
    "print(f\"Test Accuracy: {scores[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bzq modifying\n",
    "len_x = len_x_target\n",
    "len_y = len_y_target\n",
    "stride_x = stride_x_target\n",
    "stride_y = stride_y_target\n",
    "batch_size = 10000  # 設定批次大小\n",
    "\n",
    "acc_cifar = []\n",
    "\n",
    "original_predictions_cifar = []\n",
    "for start in range(0, len(preprocessed_data), batch_size):\n",
    "    end = min(start + batch_size, len(preprocessed_data))\n",
    "    batch_data = preprocessed_data[start:end]\n",
    "    batch_labels = test_labels[start:end]\n",
    "\n",
    "    batch_predictions_cifar = model.predict(batch_data, verbose=0)\n",
    "    original_predictions_cifar.append(batch_predictions_cifar)\n",
    "\n",
    "    for i in range(len(batch_data)):\n",
    "        if np.argmax(batch_predictions_cifar[i]) == np.argmax(batch_labels[i]):\n",
    "            acc_cifar.append(1)\n",
    "            correct_predictions_cifar.append(start + i)\n",
    "        else:\n",
    "            acc_cifar.append(0)\n",
    "            incorrect_predictions_cifar.append(start + i)\n",
    "\n",
    "original_predictions_cifar = np.vstack(original_predictions_cifar)\n",
    "\n",
    "print(f\"{len(correct_predictions_cifar)}, {len(incorrect_predictions_cifar)}\")\n",
    "\n",
    "print(len(correct_predictions_cifar) / (len(correct_predictions_cifar) + len(incorrect_predictions_cifar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def calculate_brier_components(predicted_probs, actual_labels, num_bins=15):\n",
    "    \"\"\" 計算 Brier Score 分解：不確定性（Uncertainty）、解析度（Resolution）、可靠性（Reliability） \"\"\"\n",
    "    #print(actual_labels)\n",
    "    N, K = actual_labels.shape  # 確保 `actual_labels` 具有 (N, K) 形狀\n",
    "\n",
    "    mean_observed = np.mean(actual_labels, axis=0)  # 計算所有樣本的均值\n",
    "\n",
    "    # **不確定性（Uncertainty）**\n",
    "    uncertainty = np.sum(mean_observed * (1 - mean_observed))\n",
    "\n",
    "    # **建立 bins**\n",
    "    bin_edges = np.linspace(0, 1, num_bins + 1)\n",
    "    bin_lowers, bin_uppers = bin_edges[:-1], bin_edges[1:]\n",
    "\n",
    "    resolution, reliability = 0.0, 0.0\n",
    "\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # **修正索引方式，使其作用於所有類別**\n",
    "        in_bin = (predicted_probs >= bin_lower) & (predicted_probs < bin_upper)  # 確保形狀為 (N, K)\n",
    "        \n",
    "        n_b = np.sum(in_bin, axis=0)  # 每個類別有多少樣本落入該區間\n",
    "\n",
    "        if np.any(n_b):  # 確保區間內有樣本\n",
    "            p_b = np.sum(predicted_probs * in_bin, axis=0) / np.maximum(n_b, 1)  # 計算 bin 內的平均機率\n",
    "            y_b = np.sum(actual_labels * in_bin, axis=0) / np.maximum(n_b, 1)  # 計算 bin 內的真實標籤均值\n",
    "        else:\n",
    "            p_b = np.zeros(K)\n",
    "            y_b = np.zeros(K)\n",
    "\n",
    "        reliability += np.sum(n_b * (p_b - y_b) ** 2)\n",
    "        resolution += np.sum(n_b * (y_b - mean_observed) ** 2)\n",
    "\n",
    "    # **歸一化 reliability & resolution**\n",
    "    reliability /= N\n",
    "    resolution /= N\n",
    "\n",
    "    # **計算 Brier Score**\n",
    "    brier_score = np.mean(np.sum((predicted_probs - actual_labels) ** 2, axis=1))\n",
    "\n",
    "    return brier_score, uncertainty, resolution, reliability\n",
    "\n",
    "\n",
    "def process_targets(batch_data, len_x, len_y, stride_x, stride_y, random_mask):\n",
    "    \"\"\" 針對單個影像生成不同遮罩區塊 \"\"\"\n",
    "    target_list = [\n",
    "        single_data_bzq_mask_preprocessing_cifar_random_global(\n",
    "            batch_data, i, j, len_x, len_y, random_mask\n",
    "        )\n",
    "        for i in range(0, 32 - len_y, stride_y)\n",
    "        for j in range(0, 32 - len_x, stride_x)\n",
    "    ]\n",
    "    return target_list\n",
    "\n",
    "\n",
    "def process_batch(preprocessed_data, test_labels, batch_size, len_x, len_y, stride_x, stride_y, random_mask, model, alpha):\n",
    "    \"\"\" 進行批次處理，提高效能，並計算 Brier Score 與其分解狀態 \"\"\"\n",
    "    bzq_scores, nll_values, brier_scores, uncertainties, resolutions, reliabilities = [], [], [], [], [], []\n",
    "\n",
    "    for start in range(0, len(preprocessed_data), batch_size):\n",
    "        end = min(start + batch_size, len(preprocessed_data))\n",
    "        batch_data = preprocessed_data[start:end]\n",
    "        batch_labels = test_labels[start:end]\n",
    "\n",
    "        for sample_data, sample_label in zip(batch_data, batch_labels):\n",
    "            targets = process_targets(sample_data, len_x, len_y, stride_x, stride_y, random_mask)\n",
    "            predictions = model.predict(np.array(targets).reshape(-1, 32, 32, 3), verbose=0)\n",
    "\n",
    "            # **擴展 sample_label，使其形狀匹配 predictions**\n",
    "            sample_label_expanded = np.tile(sample_label, (predictions.shape[0], 1))\n",
    "\n",
    "            # **索引計算**\n",
    "            max_indices = np.argmax(predictions, axis=1)\n",
    "            most_common_num, _ = Counter(max_indices).most_common(1)[0]\n",
    "\n",
    "            # **修正 Softmax 索引問題**\n",
    "            softmax_values = predictions[:, most_common_num]\n",
    "            true_label_indices = np.where(sample_label == 1)[0]\n",
    "\n",
    "            if true_label_indices.size > 0:\n",
    "                probabilities = predictions[np.arange(len(true_label_indices)), true_label_indices]\n",
    "                nll_values.append(-np.mean(np.log(probabilities + 1e-8)))\n",
    "\n",
    "            # **信心分數計算**\n",
    "            avg_softmax = np.mean(softmax_values)\n",
    "            std_softmax = np.std(softmax_values) if np.std(softmax_values) > 0 else 1e-8\n",
    "            confidence_score = alpha * avg_softmax + (1 - alpha) * (2.0 / np.pi * np.arctan(1.0 / std_softmax))\n",
    "\n",
    "            bzq_scores.append(confidence_score)\n",
    "\n",
    "            # **計算 Brier Score 與其分解狀態**\n",
    "            brier_score, uncertainty, resolution, reliability = calculate_brier_components(predictions, sample_label_expanded)\n",
    "            brier_scores.append(brier_score)\n",
    "            uncertainties.append(uncertainty)\n",
    "            resolutions.append(resolution)\n",
    "            reliabilities.append(reliability)\n",
    "\n",
    "    return (\n",
    "        np.array(bzq_scores),\n",
    "        np.array(nll_values),\n",
    "        np.array(brier_scores),\n",
    "        np.array(uncertainties),\n",
    "        np.array(resolutions),\n",
    "        np.array(reliabilities),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('bzq_cifar.npy'):\n",
    "    bzq_cifar = np.load('bzq_cifar.npy')\n",
    "else:\n",
    "    random_num_for_bzq_mask_cifar = np.random.randint(0, 256, (len_y_target, len_x_target, 3)).astype(np.float32)\n",
    "    bzq_cifar, nll_cifar, brier_cifar, uncertainties_cifar, resolutions_cifar, reliabilities_cifar = process_batch(preprocessed_data, test_labels, batch_size, len_x, len_y, stride_x, stride_y, random_num_for_bzq_mask_cifar, model, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preprocessed_data))\n",
    "\n",
    "\n",
    "result_bzq_cifar = bzq_cifar\n",
    "\n",
    "#print(result_bzq_cifar)\n",
    "\n",
    "counts, bins, patches = plt.hist(bzq_cifar, bins=bins_size)\n",
    "plt.title('Cumulative Histogram of Correct Predictions')\n",
    "plt.xlabel('bzq')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')  # 指定圖例位置\n",
    "plt.show()\n",
    "\n",
    "# 打印結果\n",
    "plt.boxplot(bzq_cifar)\n",
    "plt.show()\n",
    "\n",
    "# 繪製點狀圖\n",
    "plt.scatter(bzq_cifar, result_bzq_cifar)\n",
    "\n",
    "# 設定標題和軸標籤\n",
    "plt.title('Scatter Plot of x vs f')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f')\n",
    "\n",
    "# 顯示圖表\n",
    "plt.show()\n",
    "\n",
    "bzq_correct_cifar = np.array([bzq_cifar[i] for i in correct_predictions_cifar])\n",
    "bzq_incorrect_cifar = np.array([bzq_cifar[i] for i in incorrect_predictions_cifar])\n",
    "\n",
    "result_bzq_correct_cifar = np.array([result_bzq_cifar[i] for i in correct_predictions_cifar])\n",
    "result_bzq_incorrect_cifar = np.array([result_bzq_cifar[i] for i in incorrect_predictions_cifar])\n",
    "\n",
    "# 打印結果\n",
    "\n",
    "counts, bins, patches = plt.hist(bzq_correct_cifar, bins=bins_size)\n",
    "plt.title('Cumulative Histogram of Correct Predictions')\n",
    "plt.xlabel('bzq')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')  # 指定圖例位置\n",
    "plt.show()\n",
    "plt.boxplot(bzq_correct_cifar)\n",
    "plt.show()\n",
    "# 繪製點狀圖\n",
    "plt.scatter(bzq_correct_cifar, result_bzq_correct_cifar)\n",
    "\n",
    "# 設定標題和軸標籤\n",
    "plt.title('Scatter Plot of x vs f')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f')\n",
    "\n",
    "# 顯示圖表\n",
    "plt.show()\n",
    "\n",
    "# 打印結果\n",
    "counts, bins, patches = plt.hist(bzq_incorrect_cifar, bins=bins_size)\n",
    "plt.title('Cumulative Histogram of Incorrect Predictions')\n",
    "plt.xlabel('bzq')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')  # 指定圖例位置\n",
    "plt.show()\n",
    "plt.boxplot(bzq_incorrect_cifar)\n",
    "plt.show()\n",
    "# 繪製點狀圖\n",
    "plt.scatter(bzq_incorrect_cifar, result_bzq_incorrect_cifar)\n",
    "\n",
    "# 設定標題和軸標籤\n",
    "plt.title('Scatter Plot of x vs f')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f')\n",
    "\n",
    "# 顯示圖表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 劃出confidence-acc 圖: confidence由bzq提供，acc由該confidence數值底下預測準確的\n",
    "\n",
    "result_pred_cifar = np.ones(len(preprocessed_data)) \n",
    "for i in incorrect_predictions_cifar:\n",
    "    result_pred_cifar[i] = 0\n",
    "\n",
    "print(sum(result_pred_cifar))\n",
    "\n",
    "result_cifar_dict = {}\n",
    "for i, val in enumerate(result_bzq_cifar):\n",
    "    if val not in result_cifar_dict.keys():\n",
    "        result_cifar_dict[val] = [result_pred_cifar[i]]\n",
    "    else:\n",
    "        result_cifar_dict[val].append(result_pred_cifar[i])\n",
    "\n",
    "# 初始化信心值和準確率列表\n",
    "confidence_values = []\n",
    "accuracies = []\n",
    "element_counts = []\n",
    "\n",
    "# 計算每個信心值範圍的準確率\n",
    "for confidence in sorted(result_cifar_dict.keys(), reverse=True):\n",
    "    combined_results = []\n",
    "    for key in result_cifar_dict:\n",
    "        if key >= confidence:\n",
    "            combined_results.extend(result_cifar_dict[key])\n",
    "    element_count = len(combined_results)\n",
    "    accuracy = np.mean(combined_results)\n",
    "    confidence_values.append(confidence)\n",
    "    accuracies.append(accuracy)\n",
    "    element_counts.append(element_count)\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy (Rotated 60°)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values, element_counts, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Number of Elements (p(y|x) >= τ)')\n",
    "plt.title('Confidence Threshold vs Number of Elements')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 劃出confidence-acc 圖: confidence由bzq提供，acc由該confidence數值底下預測準確的\n",
    "\n",
    "result_pred_cifar = np.ones(len(preprocessed_data)) \n",
    "for i in incorrect_predictions_cifar:\n",
    "    result_pred_cifar[i] = 0\n",
    "\n",
    "print(sum(result_pred_cifar))\n",
    "\n",
    "result_cifar_dict = {}\n",
    "for i, val in enumerate(result_bzq_cifar):\n",
    "    if val not in result_cifar_dict.keys():\n",
    "        result_cifar_dict[val] = [result_pred_cifar[i]]\n",
    "    else:\n",
    "        result_cifar_dict[val].append(result_pred_cifar[i])\n",
    "\n",
    "# 初始化信心值和準確率列表\n",
    "confidence_values = []\n",
    "accuracies = []\n",
    "element_counts = []\n",
    "\n",
    "# 計算每個信心值範圍的準確率\n",
    "for confidence in sorted(result_cifar_dict.keys(), reverse=True):\n",
    "    combined_results = []\n",
    "    for key in result_cifar_dict:\n",
    "        if key >= confidence:\n",
    "            combined_results.extend(result_cifar_dict[key])\n",
    "    element_count = len(combined_results)\n",
    "    accuracy = np.mean(combined_results)\n",
    "    confidence_values.append(confidence)\n",
    "    accuracies.append(accuracy)\n",
    "    element_counts.append(element_count)\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy (Rotated 60°)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values, element_counts, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Number of Elements (p(y|x) >= τ)')\n",
    "plt.title('Confidence Threshold vs Number of Elements')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "confidence_values_scaled = np.array(confidence_values)\n",
    "#confidence_values_scaled = 2 / np.pi * np.arctan(confidence_values_scaled)\n",
    "#confidence_values_scaled = confidence_values_scaled * confidence_values_scaled / (1 - confidence_values_scaled * confidence_values_scaled)\n",
    "                                                                                  \n",
    "\n",
    "#print(confidence_values_scaled)\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_scaled, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy (Rotated 60°)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "confidence_values_scaled = scaler.fit_transform(np.array(confidence_values_scaled).reshape(-1, 1)).flatten()\n",
    "#print(confidence_values_scaled)\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_scaled, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy ')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_scaled, element_counts, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "#vanilla\n",
    "#original_predictions_cifar (800000, 10)\n",
    "# 初始化信心值和準確率列表\n",
    "\n",
    "# 初始化 confidence_map_vanilla 為 defaultdict\n",
    "confidence_map_vanilla = defaultdict(list)\n",
    "\n",
    "# 將預測結果和信心值存入字典\n",
    "for i, val in enumerate(original_predictions_cifar):\n",
    "    conf = np.max(val)\n",
    "    confidence_map_vanilla[conf].append(result_pred_cifar[i])\n",
    "\n",
    "print(\"finish\")\n",
    "print(len(confidence_map_vanilla))\n",
    "\n",
    "confidence_values_vanilla = []\n",
    "accuracies_vanilla = []\n",
    "element_counts_vanilla = []\n",
    "\n",
    "# 計算每個信心值範圍的準確率\n",
    "sorted_confidences = sorted(confidence_map_vanilla.keys(), reverse=True)\n",
    "combined_results_vanilla = []\n",
    "\n",
    "for confidence in sorted_confidences:\n",
    "    combined_results_vanilla.extend(confidence_map_vanilla[confidence])\n",
    "    element_count_vanilla = len(combined_results_vanilla)\n",
    "    accuracy_vanilla = np.mean(combined_results_vanilla)\n",
    "    confidence_values_vanilla.append(confidence)\n",
    "    accuracies_vanilla.append(accuracy_vanilla)\n",
    "    element_counts_vanilla.append(element_count_vanilla)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_vanilla, accuracies_vanilla, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_vanilla, element_counts_vanilla, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Number of Elements (p(y|x) >= τ)')\n",
    "plt.title('Confidence Threshold vs Number of Elements')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 假設 confidence_values_vanilla、accuracies_vanilla、confidence_values_scaled 和 accuracies 已經定義\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_vanilla, accuracies_vanilla, marker='.', linestyle='-', color='b', label='Vanilla', markersize=4)\n",
    "plt.plot(confidence_values_scaled, accuracies, marker='.', linestyle='-', color='r', label='Scaled', markersize=4)\n",
    "\n",
    "# 設定 X 軸顯示範圍: 0.0 ~ 1.0\n",
    "plt.xlim(0.0, 1.0)\n",
    "\n",
    "# 設定 Y 軸顯示範圍: 0.65 ~ 1.0\n",
    "#plt.ylim(0.65, 1.0)\n",
    "\n",
    "# 新增垂直線\n",
    "plt.axvline(x=0.6827, color='g', linestyle='--', label='x=0.6827')\n",
    "plt.axvline(x=0.9545, color='m', linestyle='--', label='x=0.9545')\n",
    "plt.axvline(x=0.9973, color='c', linestyle='--', label='x=0.9973')\n",
    "\n",
    "# 找到最接近的值\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "idx_6827_vanilla = find_nearest(confidence_values_vanilla, 0.6827)\n",
    "idx_9545_vanilla = find_nearest(confidence_values_vanilla, 0.9545)\n",
    "idx_9973_vanilla = find_nearest(confidence_values_vanilla, 0.9973)\n",
    "\n",
    "idx_6827_scaled = find_nearest(confidence_values_scaled, 0.6827)\n",
    "idx_9545_scaled = find_nearest(confidence_values_scaled, 0.9545)\n",
    "idx_9973_scaled = find_nearest(confidence_values_scaled, 0.9973)\n",
    "\n",
    "# 新增交點標記\n",
    "plt.scatter([confidence_values_vanilla[idx_6827_vanilla], confidence_values_vanilla[idx_9545_vanilla], confidence_values_vanilla[idx_9973_vanilla]], \n",
    "            [accuracies_vanilla[idx_6827_vanilla], accuracies_vanilla[idx_9545_vanilla], accuracies_vanilla[idx_9973_vanilla]], \n",
    "            color='black', zorder=5)\n",
    "plt.scatter([confidence_values_scaled[idx_6827_scaled], confidence_values_scaled[idx_9545_scaled], confidence_values_scaled[idx_9973_scaled]], \n",
    "            [accuracies[idx_6827_scaled], accuracies[idx_9545_scaled], accuracies[idx_9973_scaled]], \n",
    "            color='black', zorder=5)\n",
    "\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(accuracies[idx_6827_scaled], accuracies[idx_9545_scaled], accuracies[idx_9973_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存儲到 .npy 檔案 \n",
    "print(random_num_for_bzq_mask_cifar)\n",
    "'''np.save('confidence_values_vanilla.npy', confidence_values_vanilla) \n",
    "np.save('accuracies_vanilla.npy', accuracies_vanilla) \n",
    "np.save('element_counts_vanilla.npy', element_counts_vanilla)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.sum([item for sublist in confidence_map_vanilla.values() for item in sublist]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bzq_cifar_modified = scaler.fit_transform(np.array(result_bzq_cifar).reshape(-1, 1)).flatten()\n",
    "#result_bzq_cifar_modified = result_bzq_cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECE calc\n",
    "\n",
    "def calculate_ece(confidences, labels, num_bins=15):\n",
    "    bin_boundaries = np.linspace(0, 1, num_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = np.logical_and(confidences > bin_lower, confidences <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(labels[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "    return ece\n",
    "\n",
    "\n",
    "#print(result_bzq_cifar)\n",
    "# 計算ECE\n",
    "ece_cifar = [calculate_ece(result_bzq_cifar[limit * i : limit * (i + 1)], \n",
    "                     result_pred_cifar[limit * i : limit * (i + 1)]) \n",
    "                     for i in range(len(bzq_cifar) // limit)]\n",
    "ece_cifar_modified = [calculate_ece(result_bzq_cifar_modified[limit * i : limit * (i + 1)], \n",
    "                     result_pred_cifar[limit * i : limit * (i + 1)]) \n",
    "                     for i in range(len(bzq_cifar) // limit)]\n",
    "#print(\"Expected Calibration Error (ECE):\", ece_cifar)\n",
    "fig, ax = plt.subplots() \n",
    "ax.boxplot(ece_cifar) \n",
    "ax.set_title('ECE Boxplot') \n",
    "ax.set_ybound(0, 0.7)\n",
    "ax.set_ylabel('ECE') \n",
    "plt.show()\n",
    "#print(\"Expected Calibration Error (ECE):\", ece_cifar_modified)\n",
    "fig, ax = plt.subplots() \n",
    "ax.boxplot(ece_cifar_modified) \n",
    "ax.set_title('ECE Boxplot') \n",
    "ax.set_ybound(0, 0.7)\n",
    "ax.set_ylabel('ECE') \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_conf = []\n",
    "acc = [np.mean(acc_cifar[limit * i : limit * (i + 1)]) for i in range(len(acc_cifar) // limit)]\n",
    "print(acc)\n",
    "fig, ax = plt.subplots() \n",
    "ax.boxplot(acc) \n",
    "ax.set_title('Acc Boxplot') \n",
    "ax.set_ybound(0, 1)\n",
    "ax.set_ylabel('Acc') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = []\n",
    "bs = []\n",
    "'''nll = np.mean(nll_cifar)\n",
    "bs = np.mean(brier_cifar)'''\n",
    "\n",
    "for i in range(len(bzq_cifar) // limit):\n",
    "    nll.append(np.mean(nll_cifar[limit * i: limit * (i + 1)]))\n",
    "    bs.append(np.mean(brier_cifar[limit * i: limit * (i + 1)]))\n",
    "\n",
    "print(len(nll_cifar), len(brier_cifar), len(ece_cifar_modified))\n",
    "print(nll_cifar)\n",
    "print(brier_cifar)\n",
    "print(ece_cifar_modified)\n",
    "\n",
    "print(np.percentile(ece_cifar, 25), np.percentile(ece_cifar, 50), np.percentile(ece_cifar, 75))\n",
    "print(np.percentile(ece_cifar_modified, 25), np.percentile(ece_cifar_modified, 50), np.percentile(ece_cifar_modified, 75))\n",
    "print(np.percentile(bs, 25), np.percentile(bs, 50), np.percentile(bs, 75))\n",
    "print(np.percentile(nll, 25), np.percentile(nll, 50), np.percentile(nll, 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc, ece_cifar_modified, bs, nll, reli)\n",
    "\n",
    "\n",
    "\n",
    "idx_cifar = [np.arange(i, len(acc), 5) for i in range(5)]\n",
    "val_cifar = [[acc[idx] for idx in idx_cifar[i]] for i in range(5)]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(val_cifar)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "idx_cifar = [np.arange(i, len(ece_cifar_modified), 5) for i in range(5)]\n",
    "val_cifar = [[ece_cifar_modified[idx] for idx in idx_cifar[i]] for i in range(5)]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(val_cifar)\n",
    "plt.ylim(0, 0.7)\n",
    "plt.title(\"ECE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "idx_cifar = [np.arange(i, len(bs), 5) for i in range(5)]\n",
    "val_cifar = [[bs[idx] for idx in idx_cifar[i]] for i in range(5)]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(val_cifar)\n",
    "plt.ylim(0, 1.4)\n",
    "plt.title(\"Brier Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "idx_cifar = [np.arange(i, len(nll), 5) for i in range(5)]\n",
    "val_cifar = [[nll[idx] for idx in idx_cifar[i]] for i in range(5)]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(val_cifar)\n",
    "\n",
    "plt.title(\"NLL\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "reli = []\n",
    "resol = []\n",
    "for i in range(len(bzq_cifar) // limit):\n",
    "    reli.append(np.mean(reliabilities_cifar[limit * i: limit * (i + 1)]))\n",
    "    resol.append(np.mean(resolutions_cifar[limit * i: limit * (i + 1)]))\n",
    "\n",
    "idx_cifar = [np.arange(i, len(reli), 5) for i in range(5)]\n",
    "val_cifar = [[reli[idx] for idx in idx_cifar[i]] for i in range(5)]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(val_cifar)\n",
    "plt.ylim(0, 1.6)\n",
    "plt.title(\"Reliability\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "idx_cifar = [np.arange(i, len(reli), 5) for i in range(5)]\n",
    "val_cifar = [[resol[idx] for idx in idx_cifar[i]] for i in range(5)]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(val_cifar)\n",
    "plt.ylim(0, 1.8)\n",
    "plt.title(\"Resolution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
