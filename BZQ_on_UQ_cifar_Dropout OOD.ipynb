{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model modifying\n",
    "dropout_rate = 0.054988\n",
    "\n",
    "# bzq modifying\n",
    "len_x_target = 3\n",
    "len_y_target = 3\n",
    "stride_x_target = 1\n",
    "stride_y_target = 1\n",
    "\n",
    "# mean, std proportion\n",
    "alpha = 0.5\n",
    "\n",
    "bins_size = 30  # 統計採樣數\n",
    "poly_degree = bins_size - 1\n",
    "window_size = 1\n",
    "\n",
    "#target image preprocessing\n",
    "angle = 0\n",
    "pixels = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.interpolate import interp1d\n",
    "import tensorflow_datasets as tfds\n",
    "from collections import Counter\n",
    "import scipy.ndimage\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet-20 V1 architecture\n",
    "def resnet_block(inputs, filters, kernel_size=3, stride=1, activation='relu', dropout_rate=dropout_rate):\n",
    "    x = layers.Dropout(dropout_rate)(inputs, training=True)  # Add Dropout layer here\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Dropout(dropout_rate)(x, training=True)  # Add Dropout layer here\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if stride != 1 or inputs.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=1, strides=stride, padding='same')(inputs)\n",
    "    else:\n",
    "        shortcut = inputs\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def resnet_v1(input_shape, num_classes, dropout_rate=dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Dropout(dropout_rate)(inputs, training=True)  # Add Dropout layer here\n",
    "    x = layers.Conv2D(16, kernel_size=3, strides=1, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    for _ in range(3):\n",
    "        x = resnet_block(x, 16)\n",
    "    for _ in range(3):\n",
    "        x = resnet_block(x, 32, stride=2)\n",
    "    for _ in range(3):\n",
    "        x = resnet_block(x, 64, stride=2)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x, training=True)  # Add Dropout layer here\n",
    "    x = layers.Dense(num_classes, activation='softmax', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    model = models.Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "y_train, y_test = tf.keras.utils.to_categorical(y_train), tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# 創建 tf.data.Dataset 並加入 RandomRotation \n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.Lambda(lambda x: tf.image.random_flip_left_right(x)),\n",
    "    layers.Lambda(lambda x: tf.pad(x, [[4, 4], [4, 4], [0, 0]])), \n",
    "    layers.Lambda(lambda x: tf.image.random_crop(x, (32, 32, 3))), \n",
    "    layers.Lambda(lambda x: tf.cast(x, tf.float32)),\n",
    "    ]) # 隨機旋轉圖片\n",
    "                                         \n",
    "# Create tf.data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=50000).batch(5).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.map(lambda x, y: (tf.cast(x, tf.float32), y)).batch(5)\n",
    "\n",
    "# Define model\n",
    "model = resnet_v1(input_shape=(32, 32, 3), num_classes=10)\n",
    "\n",
    "# Compile model\n",
    "initial_lr = 0.000250\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=initial_lr),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "# Learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    lr = initial_lr\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Train model\n",
    "epochs = 200\n",
    "if os.path.exists('cifarcD.weights.h5'):\n",
    "    model.load_weights(\"cifarcD.weights.h5\")\n",
    "    model.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "else:\n",
    "    model.fit(train_dataset,\n",
    "              validation_data=test_dataset,\n",
    "              epochs=epochs,\n",
    "              callbacks=[lr_scheduler])\n",
    "    model.save_weights(\"cifarcD.weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct_predictions_cifar = []\n",
    "incorrect_predictions_cifar = []\n",
    "\n",
    "\n",
    "# 下載並準備CIFAR-10-C資料集\n",
    "def load_cifar10_c():\n",
    "    url = 'https://zenodo.org/record/2535967/files/CIFAR-10-C.tar'\n",
    "    path = tf.keras.utils.get_file('CIFAR-10-C.tar', url, untar=True)\n",
    "    return path\n",
    "\n",
    "# 載入CIFAR-10-C資料集\n",
    "def load_cifar10_c_data(data_dir):\n",
    "    corruption_types = ['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'pixelate', 'saturate', 'shot_noise', 'spatter', 'speckle_noise', 'zoom_blur']\n",
    "    #corruption_types = ['gaussian_blur']\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for corruption in corruption_types:\n",
    "        print(corruption)\n",
    "        file_path = os.path.join(data_dir.replace(\".tar\", \"\"), f'{corruption}.npy')\n",
    "        with open(file_path, 'rb') as f:\n",
    "            all_images = np.load(f)\n",
    "            images.append(all_images[20000:30000])\n",
    "            #images.append(np.load(f))\n",
    "        labels.append(np.load(os.path.join(data_dir.replace(\".tar\", \"\"), 'labels.npy'))[20000:30000])\n",
    "\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return images, labels\n",
    "\n",
    "# 下載資料集\n",
    "cifar10_c_path = load_cifar10_c()\n",
    "\n",
    "# 載入CIFAR-10-C資料集\n",
    "test_images, test_labels = load_cifar10_c_data(cifar10_c_path)\n",
    "\n",
    "test_images = np.float32(test_images)\n",
    "preprocessed_data = test_images\n",
    "\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_data.shape, test_labels.shape)\n",
    "'''\n",
    "def data_generator(preprocessed_data, test_labels):\n",
    "    for image, label in zip(preprocessed_data, test_labels):\n",
    "        yield image, label\n",
    "\n",
    "batch_size = 32  # 可以根據需要調整批次大小\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(preprocessed_data, test_labels),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(32, 32, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(10,), dtype=tf.int64)\n",
    "    )\n",
    ")\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "scores = model.evaluate(dataset)\n",
    "print(f\"Test Loss: {scores[0]}\")\n",
    "print(f\"Test Accuracy: {scores[1]}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000  # 設定批次大小\n",
    "\n",
    "original_predictions_cifar = []\n",
    "for start in range(0, len(preprocessed_data), batch_size):\n",
    "    end = min(start + batch_size, len(preprocessed_data))\n",
    "    batch_data = preprocessed_data[start:end]\n",
    "    batch_labels = test_labels[start:end]\n",
    "\n",
    "    batch_predictions_cifar = model.predict(batch_data, verbose=0)\n",
    "    original_predictions_cifar.append(batch_predictions_cifar)\n",
    "\n",
    "    for i in range(len(batch_data)):\n",
    "        if np.argmax(batch_predictions_cifar[i]) == np.argmax(batch_labels[i]):\n",
    "            correct_predictions_cifar.append(start + i)\n",
    "        else:\n",
    "            incorrect_predictions_cifar.append(start + i)\n",
    "\n",
    "original_predictions_cifar = np.vstack(original_predictions_cifar)\n",
    "\n",
    "print(f\"{len(correct_predictions_cifar)}, {len(incorrect_predictions_cifar)}\")\n",
    "\n",
    "print(len(correct_predictions_cifar) / (len(correct_predictions_cifar) + len(incorrect_predictions_cifar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MC Dropout prediction function\n",
    "def mc_dropout_predictions(model, X, n_predictions):\n",
    "    f = tf.function(lambda x: model(x, training=True))  # Keep dropout during prediction\n",
    "    predictions = [f(X) for _ in range(n_predictions)]\n",
    "    return tf.stack(predictions)\n",
    "\n",
    "mean_preds = []\n",
    "# Generate MC Dropout predictions\n",
    "for start in range(0, len(preprocessed_data), batch_size):\n",
    "    end = min(start + batch_size, len(preprocessed_data))\n",
    "    n_predictions = 128\n",
    "    batch_data = preprocessed_data[start:end]\n",
    "    preds = mc_dropout_predictions(model, batch_data, n_predictions)\n",
    "    mean_preds.extend(tf.reduce_mean(preds, axis=0))\n",
    "    #std_preds += tf.math.reduce_std(preds, axis=0)\n",
    "mean_preds = tf.stack(mean_preds)\n",
    "confidence = np.max(mean_preds, axis=1)\n",
    "print(\"平均預測值:\", confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "#vanilla\n",
    "# 初始化信心值和準確率列表\n",
    "result_pred_cifar = np.ones(len(preprocessed_data)) \n",
    "for i in incorrect_predictions_cifar:\n",
    "    result_pred_cifar[i] = 0\n",
    "\n",
    "print(sum(result_pred_cifar))\n",
    "\n",
    "# 初始化 confidence_map_vanilla 為 defaultdict\n",
    "confidence_map_vanilla = defaultdict(list)\n",
    "\n",
    "# 將預測結果和信心值存入字典\n",
    "for i, _ in enumerate(original_predictions_cifar):\n",
    "    conf = np.max(mean_preds[i])\n",
    "    confidence_map_vanilla[conf].append(result_pred_cifar[i])\n",
    "\n",
    "print(\"finish\")\n",
    "print(len(confidence_map_vanilla))\n",
    "\n",
    "confidence_values_vanilla = []\n",
    "accuracies_vanilla = []\n",
    "element_counts_vanilla = []\n",
    "\n",
    "# 計算每個信心值範圍的準確率\n",
    "sorted_confidences = sorted(confidence_map_vanilla.keys(), reverse=True)\n",
    "combined_results_vanilla = []\n",
    "\n",
    "for confidence in sorted_confidences:\n",
    "    combined_results_vanilla.extend(confidence_map_vanilla[confidence])\n",
    "    element_count_vanilla = len(combined_results_vanilla)\n",
    "    accuracy_vanilla = np.mean(combined_results_vanilla)\n",
    "    confidence_values_vanilla.append(confidence)\n",
    "    accuracies_vanilla.append(accuracy_vanilla)\n",
    "    element_counts_vanilla.append(element_count_vanilla)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_vanilla, element_counts_vanilla, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Number of Elements (p(y|x) >= τ)')\n",
    "plt.title('Confidence Threshold vs Number of Elements')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 假設 confidence_values_vanilla、accuracies_vanilla、confidence_values_scaled 和 accuracies 已經定義\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_vanilla, accuracies_vanilla, marker='.', linestyle='-', color='b', label='Vanilla', markersize=4)\n",
    "\n",
    "# 設定 X 軸顯示範圍: 0.0 ~ 1.0\n",
    "plt.xlim(0.0, 1.0)\n",
    "\n",
    "# 設定 Y 軸顯示範圍: 0.65 ~ 1.0\n",
    "plt.ylim(0.65, 1.0)\n",
    "\n",
    "# 新增垂直線\n",
    "plt.axvline(x=0.6827, color='g', linestyle='--', label='x=0.6827')\n",
    "plt.axvline(x=0.9545, color='m', linestyle='--', label='x=0.9545')\n",
    "plt.axvline(x=0.9973, color='c', linestyle='--', label='x=0.9973')\n",
    "\n",
    "# 找到最接近的值\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "idx_6827_vanilla = find_nearest(confidence_values_vanilla, 0.6827)\n",
    "idx_9545_vanilla = find_nearest(confidence_values_vanilla, 0.9545)\n",
    "idx_9973_vanilla = find_nearest(confidence_values_vanilla, 0.9973)\n",
    "\n",
    "# 新增交點標記\n",
    "plt.scatter([confidence_values_vanilla[idx_6827_vanilla], confidence_values_vanilla[idx_9545_vanilla], confidence_values_vanilla[idx_9973_vanilla]], \n",
    "            [accuracies_vanilla[idx_6827_vanilla], accuracies_vanilla[idx_9545_vanilla], accuracies_vanilla[idx_9973_vanilla]], \n",
    "            color='black', zorder=5)\n",
    "\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(accuracies_vanilla[idx_6827_vanilla], accuracies_vanilla[idx_9545_vanilla], accuracies_vanilla[idx_9973_vanilla])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ece(confidences, labels, num_bins=15):\n",
    "    bin_boundaries = np.linspace(0, 1, num_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = np.logical_and(confidences > bin_lower, confidences <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(labels[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "    return ece\n",
    "\n",
    "\n",
    "print(np.max(mean_preds, axis=1))\n",
    "# 計算ECE\n",
    "ece = [calculate_ece(np.max(mean_preds, axis=1)[10000 * i : 10000 * (i + 1)], \n",
    "                     result_pred_cifar[10000 * i : 10000 * (i + 1)]) \n",
    "                     for i in range(16)]\n",
    "\n",
    "print(\"Expected Calibration Error (ECE):\", ece)\n",
    "fig, ax = plt.subplots() \n",
    "ax.boxplot(ece) \n",
    "ax.set_title('ECE Boxplot') \n",
    "ax.set_ybound(0, 0.7)\n",
    "ax.set_ylabel('ECE') \n",
    "plt.show()\n",
    "\n",
    "print(np.percentile(ece, 25), np.percentile(ece, 50), np.percentile(ece, 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
