{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bzq modifying\n",
    "len_x_target = 3\n",
    "len_y_target = 3\n",
    "stride_x_target = 1\n",
    "stride_y_target = 1\n",
    "\n",
    "bins_size = 30  # 統計採樣數\n",
    "poly_degree = bins_size - 1\n",
    "window_size = 1\n",
    "\n",
    "#target image preprocessing\n",
    "angle = 0\n",
    "pixels = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.interpolate import interp1d\n",
    "import tensorflow_datasets as tfds\n",
    "from collections import Counter\n",
    "import scipy.ndimage\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet-20 V1 architecture\n",
    "def resnet_block(inputs, filters, kernel_size=3, stride=1, activation='relu'):\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if stride != 1 or inputs.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=1, strides=stride, padding='same')(inputs)\n",
    "    else:\n",
    "        shortcut = inputs\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def resnet_v1(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(16, kernel_size=3, strides=1, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    for _ in range(3):\n",
    "        x = resnet_block(x, 16)\n",
    "    for _ in range(3):\n",
    "        x = resnet_block(x, 32, stride=2)\n",
    "    for _ in range(3):\n",
    "        x = resnet_block(x, 64, stride=2)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    #x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    model = models.Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "'''def custom_preprocessing(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    #image = tf.pad(image, [[4, 4], [4, 4], [0, 0]])\n",
    "    #image = tf.image.random_crop(image, (32, 32, 3))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image, label'''\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "y_train, y_test = tf.keras.utils.to_categorical(y_train), tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# 創建 tf.data.Dataset 並加入 RandomRotation \n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.Lambda(lambda x: tf.image.random_flip_left_right(x)),\n",
    "    #layers.Lambda(lambda x: tf.pad(x, [[4, 4], [4, 4], [0, 0]])), \n",
    "    #layers.Lambda(lambda x: tf.image.random_crop(x, (32, 32, 3))), \n",
    "    layers.Lambda(lambda x: tf.cast(x, tf.float32)),\n",
    "    #layers.RandomRotation(1)\n",
    "    ]) # 隨機旋轉圖片\n",
    "                                         \n",
    "# Create tf.data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#train_dataset = train_dataset.map(custom_preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=50000).batch(7).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.map(lambda x, y: (tf.cast(x, tf.float32), y)).batch(7)\n",
    "\n",
    "# Define model\n",
    "model = resnet_v1(input_shape=(32, 32, 3), num_classes=10)\n",
    "\n",
    "# Compile model\n",
    "initial_lr = 0.000717\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=initial_lr),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "# Learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    lr = initial_lr\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Train model\n",
    "epochs = 200\n",
    "if os.path.exists('cifarc.weights.h5'):\n",
    "    model.load_weights(\"cifarc.weights.h5\")\n",
    "    model.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "else:\n",
    "    model.fit(train_dataset,\n",
    "              validation_data=test_dataset,\n",
    "              epochs=epochs,\n",
    "              callbacks=[lr_scheduler])\n",
    "    model.save_weights(\"cifarc.weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到了bzq 正確的函數，拿來做cifar 正確的預測\n",
    "\n",
    "def single_data_bzq_mask_preprocessing_cifar(original_data, start_x, start_y, len_x, len_y, magnification):\n",
    "    if len_x <= 0 or len_y <= 0:\n",
    "        return original_data\n",
    "    new_data = np.copy(original_data)\n",
    "    new_data[start_y:start_y + len_y, start_x:start_x + len_x, :] *= magnification\n",
    "    return new_data\n",
    "\n",
    "\n",
    "#print(random_num_for_bzq_mask_cifar)\n",
    "\n",
    "def single_data_bzq_mask_preprocessing_cifar_random_global(original_data, start_x, start_y, len_x, len_y, random_num_for_bzq_mask_cifar):\n",
    "    if len_x <= 0 or len_y <= 0:\n",
    "        return original_data\n",
    "    new_data = np.copy(original_data)\n",
    "    random_num_for_bzq_mask_cifar = random_num_for_bzq_mask_cifar[:len_y, :len_x, :] \n",
    "    new_data[start_y:start_y + len_y, start_x:start_x + len_x, :] = random_num_for_bzq_mask_cifar\n",
    "    return new_data\n",
    "\n",
    "bzq = []\n",
    "correct_predictions_cifar = []\n",
    "incorrect_predictions_cifar = []\n",
    "bzq_cifar = []\n",
    "\n",
    "# 下載並準備CIFAR-10-C資料集\n",
    "def load_cifar10_c():\n",
    "    url = 'https://zenodo.org/record/2535967/files/CIFAR-10-C.tar'\n",
    "    path = tf.keras.utils.get_file('CIFAR-10-C.tar', url, untar=True)\n",
    "    return path\n",
    "\n",
    "# 載入CIFAR-10-C資料集\n",
    "def load_cifar10_c_data(data_dir):\n",
    "    #corruption_types = ['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'jpeg_compression', 'motion_blur', 'pixelate', 'saturate', 'shot_noise', 'snow', 'spatter', 'speckle_noise', 'zoom_blur']\n",
    "    \n",
    "    corruption_types = ['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'pixelate', 'saturate', 'shot_noise', 'spatter', 'speckle_noise', 'zoom_blur']\n",
    "    #corruption_types = ['gaussian_blur']\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for corruption in corruption_types:\n",
    "        print(corruption)\n",
    "        file_path = os.path.join(data_dir.replace(\".tar\", \"\"), f'{corruption}.npy')\n",
    "        with open(file_path, 'rb') as f:\n",
    "            all_images = np.load(f)\n",
    "            images.append(all_images[20000:30000])\n",
    "            #images.append(np.load(f))\n",
    "        labels.append(np.load(os.path.join(data_dir.replace(\".tar\", \"\"), 'labels.npy'))[20000:30000])\n",
    "\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return images, labels\n",
    "\n",
    "# 下載資料集\n",
    "cifar10_c_path = load_cifar10_c()\n",
    "\n",
    "# 載入CIFAR-10-C資料集\n",
    "test_images, test_labels = load_cifar10_c_data(cifar10_c_path)\n",
    "\n",
    "test_images = np.float32(test_images)\n",
    "preprocessed_data = test_images\n",
    "\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_data.shape, test_labels.shape)\n",
    "\n",
    "def data_generator(preprocessed_data, test_labels):\n",
    "    for image, label in zip(preprocessed_data, test_labels):\n",
    "        yield image, label\n",
    "\n",
    "batch_size = 32  # 可以根據需要調整批次大小\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(preprocessed_data, test_labels),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(32, 32, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(10,), dtype=tf.int64)\n",
    "    )\n",
    ")\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "scores = model.evaluate(dataset)\n",
    "print(f\"Test Loss: {scores[0]}\")\n",
    "print(f\"Test Accuracy: {scores[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bzq modifying\n",
    "len_x = len_x_target\n",
    "len_y = len_y_target\n",
    "stride_x = stride_x_target\n",
    "stride_y = stride_y_target\n",
    "batch_size = 10000  # 設定批次大小\n",
    "\n",
    "original_predictions_cifar = []\n",
    "for start in range(0, len(preprocessed_data), batch_size):\n",
    "    end = min(start + batch_size, len(preprocessed_data))\n",
    "    batch_data = preprocessed_data[start:end]\n",
    "    batch_labels = test_labels[start:end]\n",
    "\n",
    "    batch_predictions_cifar = model.predict(batch_data, verbose=0)\n",
    "    original_predictions_cifar.append(batch_predictions_cifar)\n",
    "\n",
    "    for i in range(len(batch_data)):\n",
    "        if np.argmax(batch_predictions_cifar[i]) == np.argmax(batch_labels[i]):\n",
    "            correct_predictions_cifar.append(start + i)\n",
    "        else:\n",
    "            incorrect_predictions_cifar.append(start + i)\n",
    "\n",
    "original_predictions_cifar = np.vstack(original_predictions_cifar)\n",
    "\n",
    "print(f\"{len(correct_predictions_cifar)}, {len(incorrect_predictions_cifar)}\")\n",
    "\n",
    "print(len(correct_predictions_cifar) / (len(correct_predictions_cifar) + len(incorrect_predictions_cifar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists('bzq_cifar.npy'):\n",
    "    bzq_cifar = np.load('bzq_cifar.npy')\n",
    "else:\n",
    "    # 使用NumPy的向量化操作來生成隨機數\n",
    "    random_num_for_bzq_mask_cifar = np.random.randint(0, 256, (len_y_target, len_x_target, 3)).astype(np.float32)\n",
    "    for start in range(0, len(preprocessed_data), batch_size):\n",
    "        end = min(start + batch_size, len(preprocessed_data))\n",
    "        batch_data = preprocessed_data[start:end]\n",
    "        batch_labels = test_labels[start:end]\n",
    "\n",
    "        for k in range(len(batch_data)):\n",
    "            single_data_bzq_classification_record = []\n",
    "            targets = []\n",
    "            \n",
    "            for i in range(0, 32 - len_y, stride_y):\n",
    "                for j in range(0, 32 - len_x, stride_x):\n",
    "                    #target = single_data_bzq_mask_preprocessing_cifar(batch_data[k], i, j, len_x, len_y, 0)\n",
    "                    target = single_data_bzq_mask_preprocessing_cifar_random_global(batch_data[k], i, j, len_x, len_y, random_num_for_bzq_mask_cifar)\n",
    "                    targets.append(target)\n",
    "\n",
    "                # 批次預測\n",
    "            predictions = model.predict(np.vstack(targets).reshape(-1, 32, 32, 3), verbose=0)\n",
    "            max_bzq_indices = np.argmax(predictions, axis=1)\n",
    "                \n",
    "            single_data_bzq_classification_record.extend(max_bzq_indices)\n",
    "                \n",
    "            counter = Counter(single_data_bzq_classification_record)\n",
    "            most_common_num, most_common_count = counter.most_common(1)[0]\n",
    "                    \n",
    "            bzq.append((len(single_data_bzq_classification_record) - most_common_count) / len(single_data_bzq_classification_record))\n",
    "\n",
    "            original_data = single_data_bzq_mask_preprocessing_cifar(batch_data[k], 0, 0, 0, 0, 0)\n",
    "            original_prediction = model.predict(original_data.reshape(-1, 32, 32, 3), verbose=0)\n",
    "\n",
    "            max_original_index = np.argmax(original_prediction)\n",
    "        \n",
    "    bzq = np.array(bzq)\n",
    "    bzq_cifar = bzq\n",
    "    #np.save('bzq_cifar.npy', bzq_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preprocessed_data))\n",
    "\n",
    "\n",
    "result_bzq_cifar = 1 / bzq_cifar\n",
    "        \n",
    "#print(result_bzq_cifar)\n",
    "\n",
    "counts, bins, patches = plt.hist(bzq_cifar, bins=bins_size)\n",
    "plt.title('Cumulative Histogram of Correct Predictions')\n",
    "plt.xlabel('bzq')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')  # 指定圖例位置\n",
    "plt.show()\n",
    "\n",
    "# 打印結果\n",
    "plt.boxplot(bzq_cifar)\n",
    "plt.show()\n",
    "\n",
    "# 繪製點狀圖\n",
    "plt.scatter(bzq_cifar, result_bzq_cifar)\n",
    "\n",
    "# 設定標題和軸標籤\n",
    "plt.title('Scatter Plot of x vs f')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f')\n",
    "\n",
    "# 顯示圖表\n",
    "plt.show()\n",
    "\n",
    "bzq_correct_cifar = np.array([bzq_cifar[i] for i in correct_predictions_cifar])\n",
    "bzq_incorrect_cifar = np.array([bzq_cifar[i] for i in incorrect_predictions_cifar])\n",
    "\n",
    "result_bzq_correct_cifar = np.array([result_bzq_cifar[i] for i in correct_predictions_cifar])\n",
    "result_bzq_incorrect_cifar = np.array([result_bzq_cifar[i] for i in incorrect_predictions_cifar])\n",
    "\n",
    "# 打印結果\n",
    "\n",
    "counts, bins, patches = plt.hist(bzq_correct_cifar, bins=bins_size)\n",
    "plt.title('Cumulative Histogram of Correct Predictions')\n",
    "plt.xlabel('bzq')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')  # 指定圖例位置\n",
    "plt.show()\n",
    "plt.boxplot(bzq_correct_cifar)\n",
    "plt.show()\n",
    "# 繪製點狀圖\n",
    "plt.scatter(bzq_correct_cifar, result_bzq_correct_cifar)\n",
    "\n",
    "# 設定標題和軸標籤\n",
    "plt.title('Scatter Plot of x vs f')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f')\n",
    "\n",
    "# 顯示圖表\n",
    "plt.show()\n",
    "\n",
    "# 打印結果\n",
    "counts, bins, patches = plt.hist(bzq_incorrect_cifar, bins=bins_size)\n",
    "plt.title('Cumulative Histogram of Incorrect Predictions')\n",
    "plt.xlabel('bzq')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')  # 指定圖例位置\n",
    "plt.show()\n",
    "plt.boxplot(bzq_incorrect_cifar)\n",
    "plt.show()\n",
    "# 繪製點狀圖\n",
    "plt.scatter(bzq_incorrect_cifar, result_bzq_incorrect_cifar)\n",
    "\n",
    "# 設定標題和軸標籤\n",
    "plt.title('Scatter Plot of x vs f')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f')\n",
    "\n",
    "# 顯示圖表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 劃出confidence-acc 圖: confidence由bzq提供，acc由該confidence數值底下預測準確的\n",
    "\n",
    "result_pred_cifar = np.ones(len(preprocessed_data)) \n",
    "for i in incorrect_predictions_cifar:\n",
    "    result_pred_cifar[i] = 0\n",
    "\n",
    "print(sum(result_pred_cifar))\n",
    "\n",
    "result_cifar_dict = {}\n",
    "for i, val in enumerate(result_bzq_cifar):\n",
    "    if val not in result_cifar_dict.keys():\n",
    "        result_cifar_dict[val] = [result_pred_cifar[i]]\n",
    "    else:\n",
    "        result_cifar_dict[val].append(result_pred_cifar[i])\n",
    "\n",
    "# 初始化信心值和準確率列表\n",
    "confidence_values = []\n",
    "accuracies = []\n",
    "element_counts = []\n",
    "\n",
    "# 計算每個信心值範圍的準確率\n",
    "for confidence in sorted(result_cifar_dict.keys(), reverse=True):\n",
    "    combined_results = []\n",
    "    for key in result_cifar_dict:\n",
    "        if key >= confidence:\n",
    "            combined_results.extend(result_cifar_dict[key])\n",
    "    element_count = len(combined_results)\n",
    "    accuracy = np.mean(combined_results)\n",
    "    confidence_values.append(confidence)\n",
    "    accuracies.append(accuracy)\n",
    "    element_counts.append(element_count)\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy (Rotated 60°)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values, element_counts, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Number of Elements (p(y|x) >= τ)')\n",
    "plt.title('Confidence Threshold vs Number of Elements')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "confidence_values_scaled = np.array(confidence_values)\n",
    "confidence_values_scaled = 2 / np.pi * np.arctan(confidence_values_scaled)\n",
    "#confidence_values_scaled = confidence_values_scaled * confidence_values_scaled / (1 - confidence_values_scaled * confidence_values_scaled)\n",
    "                                                                                  \n",
    "\n",
    "#print(confidence_values_scaled)\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_scaled, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy (Rotated 60°)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "confidence_values_scaled = scaler.fit_transform(np.array(confidence_values_scaled).reshape(-1, 1)).flatten()\n",
    "#print(confidence_values_scaled)\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_scaled, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy ')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_scaled, element_counts, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "#vanilla\n",
    "#original_predictions_cifar (800000, 10)\n",
    "# 初始化信心值和準確率列表\n",
    "\n",
    "# 初始化 confidence_map_vanilla 為 defaultdict\n",
    "confidence_map_vanilla = defaultdict(list)\n",
    "\n",
    "# 將預測結果和信心值存入字典\n",
    "for i, val in enumerate(original_predictions_cifar):\n",
    "    conf = np.max(val)\n",
    "    confidence_map_vanilla[conf].append(result_pred_cifar[i])\n",
    "\n",
    "print(\"finish\")\n",
    "print(len(confidence_map_vanilla))\n",
    "\n",
    "confidence_values_vanilla = []\n",
    "accuracies_vanilla = []\n",
    "element_counts_vanilla = []\n",
    "\n",
    "# 計算每個信心值範圍的準確率\n",
    "sorted_confidences = sorted(confidence_map_vanilla.keys(), reverse=True)\n",
    "combined_results_vanilla = []\n",
    "\n",
    "for confidence in sorted_confidences:\n",
    "    combined_results_vanilla.extend(confidence_map_vanilla[confidence])\n",
    "    element_count_vanilla = len(combined_results_vanilla)\n",
    "    accuracy_vanilla = np.mean(combined_results_vanilla)\n",
    "    confidence_values_vanilla.append(confidence)\n",
    "    accuracies_vanilla.append(accuracy_vanilla)\n",
    "    element_counts_vanilla.append(element_count_vanilla)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_vanilla, accuracies_vanilla, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_vanilla, element_counts_vanilla, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Number of Elements (p(y|x) >= τ)')\n",
    "plt.title('Confidence Threshold vs Number of Elements')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 假設 confidence_values_vanilla、accuracies_vanilla、confidence_values_scaled 和 accuracies 已經定義\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_values_vanilla, accuracies_vanilla, marker='.', linestyle='-', color='b', label='Vanilla', markersize=4)\n",
    "plt.plot(confidence_values_scaled, accuracies, marker='.', linestyle='-', color='r', label='Scaled', markersize=4)\n",
    "\n",
    "# 新增垂直線\n",
    "plt.axvline(x=0.6827, color='g', linestyle='--', label='x=0.6827')\n",
    "plt.axvline(x=0.9545, color='m', linestyle='--', label='x=0.9545')\n",
    "plt.axvline(x=0.9973, color='c', linestyle='--', label='x=0.9973')\n",
    "\n",
    "# 找到最接近的值\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "idx_6827_vanilla = find_nearest(confidence_values_vanilla, 0.6827)\n",
    "idx_9545_vanilla = find_nearest(confidence_values_vanilla, 0.9545)\n",
    "idx_9973_vanilla = find_nearest(confidence_values_vanilla, 0.9973)\n",
    "\n",
    "idx_6827_scaled = find_nearest(confidence_values_scaled, 0.6827)\n",
    "idx_9545_scaled = find_nearest(confidence_values_scaled, 0.9545)\n",
    "idx_9973_scaled = find_nearest(confidence_values_scaled, 0.9973)\n",
    "\n",
    "# 新增交點標記\n",
    "plt.scatter([confidence_values_vanilla[idx_6827_vanilla], confidence_values_vanilla[idx_9545_vanilla], confidence_values_vanilla[idx_9973_vanilla]], \n",
    "            [accuracies_vanilla[idx_6827_vanilla], accuracies_vanilla[idx_9545_vanilla], accuracies_vanilla[idx_9973_vanilla]], \n",
    "            color='black', zorder=5)\n",
    "plt.scatter([confidence_values_scaled[idx_6827_scaled], confidence_values_scaled[idx_9545_scaled], confidence_values_scaled[idx_9973_scaled]], \n",
    "            [accuracies[idx_6827_scaled], accuracies[idx_9545_scaled], accuracies[idx_9973_scaled]], \n",
    "            color='black', zorder=5)\n",
    "\n",
    "plt.xlabel('Confidence Threshold (τ)')\n",
    "plt.ylabel('Accuracy (p(y|x) >= τ)')\n",
    "plt.title('Confidence vs Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存儲到 .npy 檔案 \n",
    "print(random_num_for_bzq_mask_cifar)\n",
    "'''np.save('confidence_values_vanilla.npy', confidence_values_vanilla) \n",
    "np.save('accuracies_vanilla.npy', accuracies_vanilla) \n",
    "np.save('element_counts_vanilla.npy', element_counts_vanilla)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.sum([item for sublist in confidence_map_vanilla.values() for item in sublist]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECE calc\n",
    "\n",
    "def calculate_ece(confidences, labels, num_bins=15):\n",
    "    bin_boundaries = np.linspace(0, 1, num_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = np.logical_and(confidences > bin_lower, confidences <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(labels[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "    return ece\n",
    "\n",
    "result_bzq_cifar_modified = scaler.fit_transform(np.array(2 / np.pi * np.arctan(result_bzq_cifar)).reshape(-1, 1)).flatten()\n",
    "\n",
    "# 計算ECE\n",
    "ece = [calculate_ece(result_bzq_cifar_modified[10000 * i : 10000 * (i + 1)], \n",
    "                     result_pred_cifar[10000 * i : 10000 * (i + 1)]) \n",
    "                     for i in range(16)]\n",
    "\n",
    "print(\"Expected Calibration Error (ECE):\", ece)\n",
    "fig, ax = plt.subplots() \n",
    "ax.boxplot(ece) \n",
    "ax.set_title('ECE Boxplot') \n",
    "ax.set_ybound(0, 0.7)\n",
    "ax.set_ylabel('ECE') \n",
    "plt.show()\n",
    "\n",
    "\n",
    "brier_score = [np.mean((result_bzq_cifar_modified[10000 * i : 10000 * (i + 1)] - result_pred_cifar[10000 * i : 10000 * (i + 1)]) ** 2) for i in range(16)]                     \n",
    "print(\"Brier Score:\", brier_score)\n",
    "fig, ax = plt.subplots() \n",
    "ax.boxplot(brier_score) \n",
    "ax.set_title('Brier Score Boxplot') \n",
    "ax.set_ybound(0, 1.4)\n",
    "ax.set_ylabel('Brier Score') \n",
    "plt.show()\n",
    "\n",
    "epsilon = 1e-15 \n",
    "# 防止 log(0) 的情況 \n",
    "result_bzq_cifar = np.clip(result_bzq_cifar, epsilon, 1 - epsilon) \n",
    "nll = [-np.mean(result_pred_cifar[10000 * i : 10000 * (i + 1)] * np.log(result_bzq_cifar_modified[10000 * i : 10000 * (i + 1)]) + (1 - result_pred_cifar[10000 * i : 10000 * (i + 1)]) * np.log(1 - result_bzq_cifar_modified[10000 * i : 10000 * (i + 1)])) for i in range(16)]\n",
    "print(\"NLL:\", nll)\n",
    "fig, ax = plt.subplots() \n",
    "ax.boxplot(nll) \n",
    "ax.set_title('NLL Boxplot') \n",
    "ax.set_ybound(0, 12)\n",
    "ax.set_ylabel('NLL') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
