{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.interpolate import interp1d\n",
    "import tensorflow_datasets as tfds\n",
    "from collections import Counter\n",
    "import scipy.ndimage\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "import gc\n",
    "import random\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import gaussian_kde\n",
    "from numba import jit\n",
    "from torchvision.datasets import Places365\n",
    "from tqdm import tqdm  # 導入進度條庫\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "limit = 1000\n",
    "\n",
    "# 設定 Dropout 機率\n",
    "dropout_rate = 0.1\n",
    "num_samples = 128\n",
    "\n",
    "# bzq modifying\n",
    "len_x_target = 20\n",
    "len_y_target = 20\n",
    "stride_x_target = 10\n",
    "stride_y_target = 10\n",
    "\n",
    "# mean, std proportion\n",
    "alpha = 0.5\n",
    "\n",
    "bins_size = 30  # 統計採樣數\n",
    "poly_degree = bins_size - 1\n",
    "window_size = 1\n",
    "\n",
    "#target image preprocessing\n",
    "angle = 0\n",
    "pixels = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設置設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),  # Convert to tensor (values in range 0–1)\n",
    "    transforms.Lambda(lambda x: x * 255.0),  # Scale to range 0–255\n",
    "    transforms.Lambda(lambda x: x[[2, 1, 0], ...]),  # Convert RGB to BGR\n",
    "    transforms.Normalize(mean=[103.939, 116.779, 123.68], std=[1.0, 1.0, 1.0])  # Subtract mean\n",
    "])\n",
    "\n",
    "# 建立隨機影像數據集（不依賴 Places365）\n",
    "class RandomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_images=1000, image_size=(224, 224)):\n",
    "        self.num_images = num_images\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        random_image = np.random.randint(0, 256, (self.image_size[1], self.image_size[0], 3), dtype=np.uint8)\n",
    "        image = Image.fromarray(random_image)\n",
    "        return transform(image)\n",
    "\n",
    "# 使用自定義隨機影像數據集\n",
    "dataset = RandomImageDataset(num_images=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bzq = models.vgg16(weights=False)\n",
    "\n",
    "# 修改 classifier 最後一層，適應 365 類別\n",
    "num_features = model_bzq.classifier[6].in_features\n",
    "model_bzq.classifier[6] = nn.Linear(num_features, 365)\n",
    "\n",
    "# 加載保存的權重\n",
    "if os.path.exists(\"vgg16_places365.pt\"):\n",
    "    checkpoint = torch.load(\"vgg16_places365.pt\")\n",
    "    \n",
    "    # 映射權重 (Caffe -> PyTorch)\n",
    "    mapped_state_dict_bzq = {\n",
    "        # Conv1 Layers\n",
    "        \"features.0.weight\": checkpoint[\"conv1_1.weight\"],\n",
    "        \"features.0.bias\": checkpoint[\"conv1_1.bias\"],\n",
    "        \"features.2.weight\": checkpoint[\"conv1_2.weight\"],\n",
    "        \"features.2.bias\": checkpoint[\"conv1_2.bias\"],\n",
    "        \n",
    "        # Conv2 Layers\n",
    "        \"features.5.weight\": checkpoint[\"conv2_1.weight\"],\n",
    "        \"features.5.bias\": checkpoint[\"conv2_1.bias\"],\n",
    "        \"features.7.weight\": checkpoint[\"conv2_2.weight\"],\n",
    "        \"features.7.bias\": checkpoint[\"conv2_2.bias\"],\n",
    "        \n",
    "        # Conv3 Layers\n",
    "        \"features.10.weight\": checkpoint[\"conv3_1.weight\"],\n",
    "        \"features.10.bias\": checkpoint[\"conv3_1.bias\"],\n",
    "        \"features.12.weight\": checkpoint[\"conv3_2.weight\"],\n",
    "        \"features.12.bias\": checkpoint[\"conv3_2.bias\"],\n",
    "        \"features.14.weight\": checkpoint[\"conv3_3.weight\"],\n",
    "        \"features.14.bias\": checkpoint[\"conv3_3.bias\"],\n",
    "        \n",
    "        # Conv4 Layers\n",
    "        \"features.17.weight\": checkpoint[\"conv4_1.weight\"],\n",
    "        \"features.17.bias\": checkpoint[\"conv4_1.bias\"],\n",
    "        \"features.19.weight\": checkpoint[\"conv4_2.weight\"],\n",
    "        \"features.19.bias\": checkpoint[\"conv4_2.bias\"],\n",
    "        \"features.21.weight\": checkpoint[\"conv4_3.weight\"],\n",
    "        \"features.21.bias\": checkpoint[\"conv4_3.bias\"],\n",
    "        \n",
    "        # Conv5 Layers\n",
    "        \"features.24.weight\": checkpoint[\"conv5_1.weight\"],\n",
    "        \"features.24.bias\": checkpoint[\"conv5_1.bias\"],\n",
    "        \"features.26.weight\": checkpoint[\"conv5_2.weight\"],\n",
    "        \"features.26.bias\": checkpoint[\"conv5_2.bias\"],\n",
    "        \"features.28.weight\": checkpoint[\"conv5_3.weight\"],\n",
    "        \"features.28.bias\": checkpoint[\"conv5_3.bias\"],\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        \"classifier.0.weight\": checkpoint[\"fc6.weight\"],\n",
    "        \"classifier.0.bias\": checkpoint[\"fc6.bias\"],\n",
    "        \"classifier.3.weight\": checkpoint[\"fc7.weight\"],\n",
    "        \"classifier.3.bias\": checkpoint[\"fc7.bias\"],\n",
    "        \"classifier.6.weight\": checkpoint[\"fc8a.weight\"],\n",
    "        \"classifier.6.bias\": checkpoint[\"fc8a.bias\"],\n",
    "    }\n",
    "    #print(model.features)\n",
    "\n",
    "    model_bzq.load_state_dict(mapped_state_dict_bzq, strict=False)\n",
    "\n",
    "\n",
    "\n",
    "# 初始化 VGG16，無預訓練權重\n",
    "model = models.vgg16(weights=False)\n",
    "\n",
    "# 修改 classifier 最後一層，適應 365 類別\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, 365)\n",
    "\n",
    " \n",
    "\n",
    "# 為 features 添加 Dropout\n",
    "new_features = []\n",
    "for layer in model.features:\n",
    "    new_features.append(layer)\n",
    "    if isinstance(layer, nn.ReLU):  # 在 ReLU 之後插入 Dropout\n",
    "        new_features.append(nn.Dropout(p=dropout_rate))\n",
    "\n",
    "model.features = nn.Sequential(*new_features)\n",
    "\n",
    "# 定義新的 classifier，刪除第3個和第7個 Dropout\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.1, inplace=False),  # 保留 p=0.1\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.1, inplace=False),  # 保留 p=0.1\n",
    "    nn.Linear(4096, 365)\n",
    ")\n",
    "\n",
    "# 替換原本的 classifier\n",
    "model.classifier = new_classifier\n",
    "\n",
    "# 加載保存的權重\n",
    "if os.path.exists(\"vgg16_places365.pt\"):\n",
    "    checkpoint = torch.load(\"vgg16_places365.pt\")\n",
    "    \n",
    "    # 映射權重 (Caffe -> PyTorch)\n",
    "    mapped_state_dict = {  \n",
    "        \"features.0.weight\": checkpoint[\"conv1_1.weight\"],\n",
    "        \"features.0.bias\": checkpoint[\"conv1_1.bias\"],\n",
    "        \"features.3.weight\": checkpoint[\"conv1_2.weight\"],\n",
    "        \"features.3.bias\": checkpoint[\"conv1_2.bias\"],\n",
    "        \"features.7.weight\": checkpoint[\"conv2_1.weight\"],\n",
    "        \"features.7.bias\": checkpoint[\"conv2_1.bias\"],\n",
    "        \"features.10.weight\": checkpoint[\"conv2_2.weight\"],\n",
    "        \"features.10.bias\": checkpoint[\"conv2_2.bias\"],\n",
    "        \"features.14.weight\": checkpoint[\"conv3_1.weight\"],\n",
    "        \"features.14.bias\": checkpoint[\"conv3_1.bias\"],\n",
    "        \"features.17.weight\": checkpoint[\"conv3_2.weight\"],\n",
    "        \"features.17.bias\": checkpoint[\"conv3_2.bias\"],\n",
    "        \"features.20.weight\": checkpoint[\"conv3_3.weight\"],\n",
    "        \"features.20.bias\": checkpoint[\"conv3_3.bias\"],\n",
    "        \"features.24.weight\": checkpoint[\"conv4_1.weight\"],\n",
    "        \"features.24.bias\": checkpoint[\"conv4_1.bias\"],\n",
    "        \"features.27.weight\": checkpoint[\"conv4_2.weight\"],\n",
    "        \"features.27.bias\": checkpoint[\"conv4_2.bias\"],\n",
    "        \"features.30.weight\": checkpoint[\"conv4_3.weight\"],\n",
    "        \"features.30.bias\": checkpoint[\"conv4_3.bias\"],\n",
    "        \"features.34.weight\": checkpoint[\"conv5_1.weight\"],\n",
    "        \"features.34.bias\": checkpoint[\"conv5_1.bias\"],\n",
    "        \"features.37.weight\": checkpoint[\"conv5_2.weight\"],\n",
    "        \"features.37.bias\": checkpoint[\"conv5_2.bias\"],\n",
    "        \"features.40.weight\": checkpoint[\"conv5_3.weight\"],\n",
    "        \"features.40.bias\": checkpoint[\"conv5_3.bias\"],\n",
    "        \"classifier.0.weight\": checkpoint[\"fc6.weight\"],\n",
    "        \"classifier.0.bias\": checkpoint[\"fc6.bias\"],\n",
    "        \"classifier.3.weight\": checkpoint[\"fc7.weight\"],\n",
    "        \"classifier.3.bias\": checkpoint[\"fc7.bias\"],\n",
    "        \"classifier.6.weight\": checkpoint[\"fc8a.weight\"],\n",
    "        \"classifier.6.bias\": checkpoint[\"fc8a.bias\"],\n",
    "    }\n",
    "    #print(model.features)\n",
    "\n",
    "    model.load_state_dict(mapped_state_dict, strict=False)\n",
    "\n",
    "# **啟用 MC Dropout**\n",
    "def enable_dropout(m):\n",
    "    if isinstance(m, nn.Dropout):\n",
    "        m.train()\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model_bzq = model_bzq.to(device)\n",
    "if dropout_rate > 0.0:\n",
    "    print(\"enable dropout!\")\n",
    "    model.apply(enable_dropout)  # 讓 Dropout 在推論時啟用\n",
    "model.eval()  # 設定評估模式，但 Dropout 仍作用\n",
    "model_bzq.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到了bzq 正確的函數，拿來做imagenet 正確的預測\n",
    "\n",
    "def single_data_bzq_mask_preprocessing_imagenet(original_data, start_x, start_y, len_x, len_y, magnification):\n",
    "    if len_x <= 0 or len_y <= 0:\n",
    "        return original_data\n",
    "    new_data = np.copy(original_data)\n",
    "    new_data[:, start_y:start_y + len_y, start_x:start_x + len_x] *= magnification\n",
    "    #new_data[start_y:start_y + len_y, start_x:start_x + len_x, :] *= magnification\n",
    "    return new_data\n",
    "\n",
    "\n",
    "#print(random_num_for_bzq_mask_imagenet)\n",
    "\n",
    "def single_data_bzq_mask_preprocessing_imagenet_random_global(original_data, start_x, start_y, len_x, len_y, random_num_for_bzq_mask_imagenet):\n",
    "    if len_x <= 0 or len_y <= 0:\n",
    "        return original_data\n",
    "    new_data = np.copy(original_data.cpu().numpy())\n",
    "    new_data = new_data.squeeze(0)\n",
    "    random_num_for_bzq_mask_imagenet = random_num_for_bzq_mask_imagenet[:, :len_y, :len_x] \n",
    "    new_data[:, start_y:start_y + len_y, start_x:start_x + len_x] = random_num_for_bzq_mask_imagenet\n",
    "    return new_data\n",
    "\n",
    "\n",
    "bzq = []\n",
    "correct_predictions_imagenet = []\n",
    "incorrect_predictions_imagenet = []\n",
    "bzq_imagenet = []\n",
    "\n",
    "nll_bzq = []\n",
    "\n",
    "#bzq = 0的時候，提取mask之下的softmax\n",
    "correct_predictions_bzq_zero_softmax_mean = []\n",
    "correct_predictions_bzq_zero_softmax_std = []\n",
    "incorrect_predictions_bzq_zero_softmax_mean = []\n",
    "incorrect_predictions_bzq_zero_softmax_std = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################\n",
    "def reverse_normalize(image, mean, std):\n",
    "    # 確保均值和標準差是 numpy array，以支持逐像素計算\n",
    "    mean = np.array(mean).reshape(1, 1, -1)  # [1, 1, C]\n",
    "    std = np.array(std).reshape(1, 1, -1)    # [1, 1, C]\n",
    "\n",
    "    # 反標準化公式\n",
    "    image = (image * std) + mean\n",
    "    return np.clip(image, 0, 255)  # 確保範圍在 [0, 255]\n",
    "\n",
    "def normalize(image, mean, std):\n",
    "    # 將影像標準化\n",
    "    mean = np.array(mean).reshape(1, 1, -1)  # [1, 1, C]\n",
    "    std = np.array(std).reshape(1, 1, -1)    # [1, 1, C]\n",
    "    image = (image - mean) / std\n",
    "    return image\n",
    "\n",
    "# Custom dataset to apply elastic_transform\n",
    "# 自定義數據集\n",
    "class IndexBasedTransformDataset(Dataset):\n",
    "    def __init__(self, original_dataset, transforms_by_range, repeat_factor=1):\n",
    "        \"\"\"\n",
    "        original_dataset: 原始 PyTorch Dataset\n",
    "        transforms_by_range: 列表，每個元素是 (start_index, end_index, transform_function)\n",
    "        repeat_factor: 數據集重複的倍數\n",
    "        \"\"\"\n",
    "        self.original_dataset = original_dataset\n",
    "        self.transforms_by_range = transforms_by_range\n",
    "        self.repeat_factor = repeat_factor  # 數據集重複幾次\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset) * self.repeat_factor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 對數據索引進行取模，將重複部分對應回原始數據\n",
    "        original_idx = idx % len(self.original_dataset)\n",
    "        image, label = self.original_dataset[original_idx]\n",
    "        \n",
    "        # 轉換影像形狀為 [H, W, C]\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.permute(1, 2, 0).numpy()  # 從 [C, H, W] 變為 [H, W, C]\n",
    "        \n",
    "        # Step 1: 反標準化\n",
    "        image = reverse_normalize(image, mean=[103.939, 116.779, 123.68], std=[1.0, 1.0, 1.0])\n",
    "\n",
    "        # Step 2: 影像增強\n",
    "        for start, end, transform in self.transforms_by_range:\n",
    "            if start <= idx < end:\n",
    "                if transform is not None:\n",
    "                    image = transform(image)\n",
    "                break\n",
    "        \n",
    "        # Step 3: 標準化\n",
    "        image = normalize(image, mean=[103.939, 116.779, 123.68], std=[1.0, 1.0, 1.0])\n",
    "\n",
    "        # 還原影像形狀為 [C, H, W]\n",
    "        image = np.transpose(image, (2, 0, 1))  # 從 [H, W, C] 變回 [C, H, W]\n",
    "        return image, label\n",
    "    \n",
    "test_dataset = dataset\n",
    "image = test_dataset[0]\n",
    "print(image.shape)\n",
    "# 創建 DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=6, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_vanilla = []\n",
    "# 預測並顯示分數\n",
    "print(test_dataset[0].shape)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "'''fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "class_names = {0:'brightness', 1:'contrast', 2:'defocus_blur', 3:'elastic_transform', 4:'fog', \n",
    "                    5:'gaussian_blur', 6:'gaussian_noise', 7:'glass_blur', 8:'impulse_noise', \n",
    "                    9:'pixelate', 10:'saturate', 11:'shot_noise', 12:'spatter', 13:'speckle_noise', 14:'zoom_blur'}  # 你的類別名稱\n",
    "for i in range(15):\n",
    "    index = limit * i  # 計算索引\n",
    "    img, _ = test_dataset[index]  # 讀取影像\n",
    "\n",
    "    # 反正規化影像\n",
    "    img = reverse_normalize(img.transpose(1, 2, 0), mean=[103.939, 116.779, 123.68], std=[1.0, 1.0, 1.0]) / 255\n",
    "    \n",
    "    ax = axes[i // 5, i % 5]  # 計算子圖位置\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{class_names[i]}\", fontsize=12)\n",
    "    ax.axis(\"off\")  # 隱藏座標軸\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bzq modifying\n",
    "len_x = len_x_target\n",
    "len_y = len_y_target\n",
    "stride_x = stride_x_target\n",
    "stride_y = stride_y_target\n",
    "batch_size = 1  # 設定批次大小\n",
    "\n",
    "original_predictions_imagenet = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data in test_loader:\n",
    "        batch_data = batch_data.to(device).float()\n",
    "        batch_predictions_imagenet = model(batch_data).cpu().numpy()\n",
    "        original_predictions_imagenet.append(batch_predictions_imagenet)\n",
    "\n",
    "original_predictions_imagenet = np.vstack(original_predictions_imagenet)\n",
    "\n",
    "# 使用正確的方式來訪問 test_dataset 中的標籤\n",
    "test_labels = [test_dataset[i][1] for i in range(len(test_dataset))]\n",
    "\n",
    "predicted_labels = np.argmax(original_predictions_imagenet, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "random_num_for_bzq_mask_imagenet = np.random.randint(0, 256, (3, len_y_target, len_x_target)).astype(np.float32) / 255.0\n",
    "\n",
    "def process_batch(batch_data, len_y, stride_y, len_x, stride_x, device, model, model_bzq, alpha, bzq, mc_samples):\n",
    "    if len(batch_data) == 0:\n",
    "        return bzq, np.array([])\n",
    "\n",
    "    targets = []\n",
    "\n",
    "    for i in range(0, 224 - len_y, stride_y):\n",
    "        for j in range(0, 224 - len_x, stride_x):\n",
    "            #target = single_data_bzq_mask_preprocessing_imagenet(batch_data, i, j, len_x, len_y, 0)\n",
    "            target = single_data_bzq_mask_preprocessing_imagenet_random_global(batch_data, i, j, len_x, len_y, random_num_for_bzq_mask_imagenet)\n",
    "            targets.append(target)\n",
    "    \n",
    "    targets_tensor = torch.from_numpy(np.vstack(targets).reshape(-1, 3, 224, 224)).float().to(device)\n",
    "    predictions = model_bzq(targets_tensor)\n",
    "    \n",
    "    #temp, _ = torch.max(F.softmax(predictions), dim=1)\n",
    "    #temp = temp.cpu().numpy()\n",
    "    \n",
    "    max_bzq_indices = torch.argmax(predictions, dim=1).cpu().numpy()\n",
    "    \n",
    "    # 計算 softmax\n",
    "    softmax_predictions = F.softmax(predictions, dim=1)\n",
    "\n",
    "    counter = Counter(max_bzq_indices)\n",
    "    most_common_num, most_common_count = counter.most_common(1)[0]\n",
    "    \n",
    "    temp = softmax_predictions[:, most_common_num].cpu().numpy()\n",
    "    \n",
    "    mc_predictions = []\n",
    "    \n",
    "    for _ in range(mc_samples):  # **進行多次 forward**\n",
    "        predictions = model(batch_data.float())\n",
    "        mc_predictions.append(predictions.unsqueeze(0))  # **儲存 MC Dropout 結果**\n",
    "\n",
    "    # **堆疊所有 MC Dropout 結果**\n",
    "    mc_predictions = torch.cat(mc_predictions, dim=0)  # (mc_samples, batch_size, num_classes)\n",
    "\n",
    "    # **計算 MC Dropout 之後的平均**\n",
    "    mean_predictions = mc_predictions.mean(dim=0)  # 均值 (batch_size, num_classes)\n",
    "\n",
    "    # **計算 softmax**\n",
    "    softmax_predictions_dropout = F.softmax(mean_predictions, dim=1)\n",
    "    \n",
    "    temp_dropout = softmax_predictions_dropout[:, most_common_num].cpu().numpy()\n",
    "    \n",
    "    bzq.append((alpha * np.mean(temp) + (1 - alpha) * (2.0 / np.pi * np.arctan(1.0 / np.std(temp)))))\n",
    "    #print(softmax_predictions.shape)\n",
    "    #print(softmax_predictions_dropout.shape)\n",
    "    # 將兩個 softmax 結果相加除以 2\n",
    "    mixed_soft_predictions = (softmax_predictions.mean(dim=0, keepdim=True) + softmax_predictions_dropout) / 2  # shape: (batch_size, num_classes)\n",
    "    entropy = -torch.sum(mixed_soft_predictions * torch.log(mixed_soft_predictions + 1e-10), dim=1)  # shape: (batch_size,)\n",
    "    return bzq, temp, temp_dropout, most_common_num, entropy.cpu().numpy()\n",
    "\n",
    "score_dropout = []\n",
    "index_count = 0\n",
    "entropies = []\n",
    "with torch.no_grad():\n",
    "    for batch_data in test_loader:\n",
    "        batch_data = batch_data.to(device)  # 將批次影像搬到指定裝置\n",
    "        if len(batch_data) > 0:\n",
    "            bzq, temp, temp_dropout, predicted_label, entropy = process_batch(\n",
    "                batch_data, len_y, stride_y, len_x, stride_x, device, model, model_bzq, alpha, bzq, num_samples\n",
    "            )\n",
    "\n",
    "            score_dropout.append(temp_dropout)\n",
    "            entropies.append(entropy)\n",
    "            \n",
    "            # 影像預處理\n",
    "            original_data = single_data_bzq_mask_preprocessing_imagenet(batch_data, 0, 0, 0, 0, 0)\n",
    "            original_prediction = model(torch.tensor(original_data.reshape(-1, 3, 224, 224)).float().to(device))\n",
    "            max_original_index = torch.argmax(original_prediction).item()\n",
    "\n",
    "            # 根據 `bzq` 值分類統計\n",
    "            if bzq[-1] == 0.0:\n",
    "                if (len(bzq) - 1) in correct_predictions_imagenet:\n",
    "                    correct_predictions_bzq_zero_softmax_mean.append(np.mean(temp))\n",
    "                    correct_predictions_bzq_zero_softmax_std.append(np.std(temp))\n",
    "                else:\n",
    "                    incorrect_predictions_bzq_zero_softmax_mean.append(np.mean(temp))\n",
    "                    incorrect_predictions_bzq_zero_softmax_std.append(np.std(temp))\n",
    "\n",
    "bzq_imagenet = np.array(bzq)\n",
    "score_dropout = np.array(score_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = np.array(entropies).flatten()\n",
    "#print(entropies)\n",
    "# 計算熵的分佈（分桶）\n",
    "bins = np.linspace(min(entropies), max(entropies), 15) \n",
    "\n",
    "hist, bin_edges = np.histogram(entropies, bins=bins)  # 計算直方圖數據\n",
    "print(bin_edges[:-1], hist)\n",
    "# 轉換為折線圖\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(bin_edges[:-1], hist, linestyle='-', marker='o')\n",
    "\n",
    "# 設定標題與軸標籤\n",
    "plt.title(\"Entropy\")\n",
    "plt.xlabel(\"Entropy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
